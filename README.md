<!-- Source: cluster/README.md -->

## Setup

### 1. Transfer Code to Cluster

```bash
# From your local machine, sync code to cluster
rsync -avz --exclude='data/' --exclude='runs/' --exclude='mlruns/' \
  --exclude='.git/' --exclude='__pycache__/' \
  ~/Desktop/MasterThesis-1/ <your-username>@cluster.address:/path/to/your/workspace/
```

Or use Git:
```bash
# On cluster
git clone <your-repo-url>
cd MasterThesis-1
```

### 2. Setup Conda Environment on Cluster

```bash
# Create and activate environment
conda create -n thesis python=3.10
conda activate thesis

# Install PyTorch with CUDA support
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Install other dependencies
pip install -r requirements.txt
```

### 3. Prepare Data

Make sure your `data/` directory is uploaded to the cluster:
```bash
# Check if data exists
python scripts/check_data_paths.py

# If needed, transfer data
rsync -avz data/ <your-username>@cluster:~/MasterThesis-1/data/
```

### 4. Create Log Directory

```bash
mkdir -p cluster/logs
```

### 5. Update Email in Scripts

Edit each `.sh` file and replace `<your-mail>` with your actual email:
```bash
# Example
#SBATCH --mail-user=your.name@uni-hildesheim.de
```

## Available Scripts

### Single Job Scripts

Run one dataset/function combination:

**IC-FLD:**
```bash
# Usage: sbatch cluster/submit_icfld_single.sh [DATASET] [FUNCTION]
sbatch cluster/submit_icfld_single.sh physionet L
sbatch cluster/submit_icfld_single.sh mimic Q
sbatch cluster/submit_icfld_single.sh ushcn S
```

**FLD:**
```bash
# Usage: sbatch cluster/submit_fld_single.sh [DATASET] [FUNCTION]
sbatch cluster/submit_fld_single.sh physionet L
sbatch cluster/submit_fld_single.sh activity C
```

### Array Job Scripts

Run all 16 configurations (4 datasets √ó 4 functions) in parallel:

**IC-FLD (all 16 configs):**
```bash
sbatch cluster/submit_icfld_array.sh
```

**FLD (all 16 configs):**
```bash
sbatch cluster/submit_fld_array.sh
```

Array jobs will automatically distribute the 16 configurations across available GPUs.

### Hyperparameter Sweep Scripts

Run Optuna-based hyperparameter search to find optimal configurations:

**IC-FLD sweep:**
```bash
# Usage: sbatch cluster/submit_icfld_hypersweep.sh [TRIALS]
sbatch cluster/submit_icfld_hypersweep.sh 20  # 20 trials per config
```

**USHCN sweep (both trainers):**
```bash
# Runs both IC-FLD and FLD sweeps on USHCN dataset
sbatch cluster/submit_ushcn_hypersweep.sh 20
```

**FLD sweep:**
```bash
# Note: Requires creating scripts/optuna_fld.py first
sbatch cluster/submit_fld_hypersweep.sh 20
```

The sweep will search over:
- Latent dimensions: [32, 128, 256, 512]
- Number of heads: [4, 8]
- Depth: [2, 4]
- Embedding per head: [2, 4, 8]

Results are logged to MLflow in `mlruns_cluster_sweep_*` directories.

## Job Configuration

### Resource Allocation

All scripts request:
- **Partition:** STUD (student partition)
- **GPUs:** 1 GPU per job
- **CPUs:** 4 cores per job
- **Memory:** 16GB per job
- **Time limits:**
  - IC-FLD: 24 hours (longer epochs)
  - FLD: 12 hours (fewer epochs)

### Hyperparameters

**IC-FLD:**
- Latent dim: 128
- Num heads: 4
- Embedding dim: 16 (4 heads √ó 4 per head)
- Depth: 2
- Epochs: 1000, Early stop: 15
- Learning rate: 1e-4, Weight decay: 1e-3

**FLD:**
- Latent dim: 20 (hardcoded)
- Num heads: 4
- Embedding dim: 4 per head
- Depth: 2
- Epochs: 300, Early stop: 30
- Learning rate: 1e-4, Weight decay: 1e-3

## Monitoring Jobs

### Check Job Status

```bash
# View your queued/running jobs
squeue -u $USER

# View detailed job info
scontrol show job <job_id>

# View array job status
squeue -u $USER --array
```

### View Logs

```bash
# Tail single job logs
tail -f cluster/logs/icfld_icfld_train_<job_id>.log
tail -f cluster/logs/fld_fld_train_<job_id>.log

# View array job logs
tail -f cluster/logs/icfld_array_<job_id>_<task_id>.log
tail -f cluster/logs/fld_array_<job_id>_<task_id>.log

# Check for errors
grep -i error cluster/logs/*.err
```

### Cancel Jobs

```bash
# Cancel a single job
scancel <job_id>

# Cancel all your jobs
scancel -u $USER

# Cancel specific array tasks
scancel <job_id>_[0-3]  # Cancel tasks 0-3
```

## Retrieving Results

### Download Results from Cluster

```bash
# From your local machine
# Download model checkpoints
rsync -avz <username>@cluster:~/MasterThesis-1/FLD_ICC/saved_models/ ./results/icfld/
rsync -avz <username>@cluster:~/MasterThesis-1/FLD/saved_models/ ./results/fld/

# Download TensorBoard logs
rsync -avz <username>@cluster:~/MasterThesis-1/runs/cluster_icfld/ ./runs/cluster_icfld/
rsync -avz <username>@cluster:~/MasterThesis-1/runs/cluster_fld/ ./runs/cluster_fld/

# Download job logs
rsync -avz <username>@cluster:~/MasterThesis-1/cluster/logs/ ./cluster/logs/
```

### View TensorBoard

```bash
# On your local machine after downloading results
tensorboard --logdir runs/cluster_icfld
tensorboard --logdir runs/cluster_fld --port 6007
```

## Troubleshooting

### Common Issues

**1. Conda not found:**
```bash
# Add conda to your path
echo 'eval "$(conda shell.bash hook)"' >> ~/.bashrc
source ~/.bashrc
```

**2. GPU not available:**
```bash
# Check GPU allocation in your job
scontrol show job <job_id> | grep TRES
nvidia-smi  # Inside a running job
```

**3. Out of memory:**
- Reduce `--batch-size` in the scripts (default: 32)
- Request more memory: `#SBATCH --mem=32G`

**4. Time limit exceeded:**
- Increase time limit: `#SBATCH --time=48:00:00`
- Enable checkpointing with `--resume` flag

**5. Dataset not found:**
```bash
# Run from cluster
python scripts/check_data_paths.py
```

### Job Efficiency Tips

1. **Use array jobs** for running multiple configurations (more efficient)
2. **Monitor early jobs** before submitting all 16 configs
3. **Check GPU utilization:**
   ```bash
   # In running job
   watch -n 1 nvidia-smi
   ```
4. **Use TensorBoard** to monitor training progress in real-time

## Example Workflow

Complete workflow for running all experiments:

```bash
# 1. Submit IC-FLD array job (16 configs)
sbatch cluster/submit_icfld_array.sh
# Note the job ID, e.g., 12345

# 2. Submit FLD array job (16 configs)
sbatch cluster/submit_fld_array.sh
# Note the job ID, e.g., 12346

# 3. Monitor progress
watch -n 30 'squeue -u $USER'

# 4. Check logs periodically
tail -f cluster/logs/icfld_array_12345_*.log
tail -f cluster/logs/fld_array_12346_*.log

# 5. After completion, download results
rsync -avz cluster:~/MasterThesis-1/FLD_ICC/saved_models/ ./results/
rsync -avz cluster:~/MasterThesis-1/runs/cluster_* ./runs/
```

## Cluster-Specific Notes

- **Storage quota:** Check your disk usage with `quota -s`
- **Job priority:** STUD partition jobs may have lower priority
- **GPU types:** Check available GPUs with `sinfo -o "%N %G"`
- **Maintenance windows:** Check cluster announcements for scheduled downtime

## Support

- Cluster documentation: https://www.uni-hildesheim.de/gitlab/ismll/cluster-tutorial
- For cluster issues: Contact your cluster administrators
- For code issues: review the repository documentation

---

<!-- Source: cluster/QUICKSTART.md -->

# Cluster Quick Start Guide

## Initial Setup (Do Once)

### 1. Transfer Code to Cluster
```bash
# Option A: Using rsync (from local machine)
rsync -avz --exclude='data/' --exclude='runs/' --exclude='mlruns/' \
  --exclude='.git/' --exclude='__pycache__/' \
  ~/Users/ixdlab/MasterThesis-1/ home/asadi/MasterThesis-1/

# Option B: Using Git (on cluster)
git clone <your-repo-url>
cd MasterThesis-1
```

### 2. Setup Environment (on cluster)
```bash
# Create conda environment
conda create -n thesis python=3.11
conda activate thesis

# Install PyTorch with CUDA
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# Install dependencies
pip install -r requirements.txt
```

### 3. Transfer Data
```bash
# From local machine
rsync -avz data/ <username>@cluster.ismll.de:~/MasterThesis-1/data/

# Verify on cluster
python scripts/check_data_paths.py
```

### 4. Update Email in Scripts
Edit all `cluster/*.sh` files and replace `<your-mail>` with your email.

## Running Experiments

### Quick Commands

**Option 1: Fixed Hyperparameters (Fast)**

Run ALL 16 configurations with reasonable defaults:
```bash
sbatch cluster/submit_icfld_array.sh  # IC-FLD (16 configs)
sbatch cluster/submit_fld_array.sh    # FLD (16 configs)
```

Run a single configuration:
```bash
sbatch cluster/submit_icfld_single.sh physionet L
sbatch cluster/submit_fld_single.sh mimic Q
```

**Option 2: Hyperparameter Search (Slow but Optimal)**

Run Optuna sweep to find best hyperparameters:
```bash
# IC-FLD sweep: 4 configs √ó 20 trials = 80 total runs
sbatch cluster/submit_icfld_hypersweep.sh 20

# USHCN sweep: both trainers (IC-FLD + FLD)
sbatch cluster/submit_ushcn_hypersweep.sh 20

# Note: FLD sweep needs optuna_fld.py script (not yet created)
# sbatch cluster/submit_fld_hypersweep.sh 20
```

### Monitor Progress

```bash
# Check job queue
squeue -u $USER

# Watch job status (updates every 30 seconds)
watch -n 30 'squeue -u $USER'

# View logs in real-time
tail -f cluster/logs/icfld_array_<jobid>_*.log

# Check for errors
grep -i error cluster/logs/*.err
```

### Cancel Jobs

```bash
# Cancel a specific job
scancel <job_id>

# Cancel all your jobs
scancel -u $USER
```

## After Completion

### Download Results

```bash
# From your local machine
# Download model checkpoints
rsync -avz <username>@cluster.ismll.de:~/MasterThesis-1/FLD_ICC/saved_models/ ./results/icfld/
rsync -avz <username>@cluster.ismll.de:~/MasterThesis-1/FLD/saved_models/ ./results/fld/

# Download TensorBoard logs
rsync -avz <username>@cluster.ismll.de:~/MasterThesis-1/runs/cluster_* ./runs/

# Download job logs
rsync -avz <username>@cluster.ismll.de:~/MasterThesis-1/cluster/logs/ ./cluster/logs/
```

### View Results

```bash
# View TensorBoard logs locally
tensorboard --logdir runs/cluster_icfld_array
tensorboard --logdir runs/cluster_fld_array --port 6007
```

## Configuration Matrix

Each array job runs these 16 combinations:

| Task ID | Dataset    | Function | Obs Time |
|---------|------------|----------|----------|
| 0       | physionet  | C        | 24       |
| 1       | physionet  | L        | 24       |
| 2       | physionet  | Q        | 24       |
| 3       | physionet  | S        | 24       |
| 4       | mimic      | C        | 24       |
| 5       | mimic      | L        | 24       |
| 6       | mimic      | Q        | 24       |
| 7       | mimic      | S        | 24       |
| 8       | activity   | C        | 3000     |
| 9       | activity   | L        | 3000     |
| 10      | activity   | Q        | 3000     |
| 11      | activity   | S        | 3000     |
| 12      | ushcn      | C        | 24       |
| 13      | ushcn      | L        | 24       |
| 14      | ushcn      | Q        | 24       |
| 15      | ushcn      | S        | 24       |

## Estimated Run Times

**Per configuration:**
- IC-FLD: ~8-12 hours (up to 1000 epochs with early stopping)
- FLD: ~4-8 hours (up to 300 epochs with early stopping)

**Full array job (16 configs):**
- If 16 GPUs available: ~8-12 hours (parallel)
- If 4 GPUs available: ~32-48 hours (4 batches)
- If 1 GPU available: ~128-192 hours (sequential)

## Common Issues

**Job fails immediately:**
- Check email in scripts is correct
- Verify conda environment exists: `conda env list`
- Check data exists: `python scripts/check_data_paths.py`

**Out of memory:**
- Reduce batch size in scripts (change `BATCH_SIZE=32` to `BATCH_SIZE=16`)

**Time limit exceeded:**
- Increase time limit: Change `#SBATCH --time=24:00:00` to `48:00:00`

**GPU not allocated:**
- Check partition has GPUs: `sinfo -o "%P %G"`
- Verify your account can request GPUs

## Support

- Detailed guide: `cluster/README.md`
- Code documentation: see repository docs
- Cluster docs: https://www.uni-hildesheim.de/gitlab/ismll/cluster-tutorial

---

<!-- Source: README.md -->

# IC-FLD Thesis Workspace

This repository gathers the codebase I use to analyse Inter-Channel Functional Latent Dynamics (IC-FLD) and to compare it with the original FLD baseline and several reference models. Everything shares the same preprocessing (same as T-PatchGNN) pipeline and logging conventions so that sweeps and final training runs are directly comparable.

## Repository layout
- `FLD_ICC/` ‚Äì IC-FLD trainers and model definitions. The generic entry point is `train_FLD_ICC.py`; `train_FLD_ICC_ushcn.py` implements the additional safeguards required by USHCN.
- `FLD/`, `Grafiti/`, `mtan/`, `tPatchGNN/` ‚Äì baseline implementations that consume the processed datasets provided by `lib/`.
- `scripts/optuna_icfld.py` ‚Äì deterministic grid-search sweep covering all datasets/functions with the discrete search space used in the original FLD study (latent dimension, heads, depth, embedding per head; with fixed `lr=1e-4`, `wd=1e-3`).
- `scripts/optuna_ushcn.py` ‚Äì grid-search driver focused on USHCN, capable of routing runs either to the IC-FLD trainer or to the classic FLD baseline.
- `scripts/export_best_hparams.py` ‚Äì legacy MLflow exporter (kept for backward compatibility; requires MLflow if you still log runs that way).
- `scripts/run_best_hparams.sh` / `.ps1` ‚Äì replay scripts that read the JSON summary, dispatch each dataset/function to the correct trainer (including the USHCN-specific variant), and log TensorBoard traces.

## Reproducing experiments

### Environment
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Single IC-FLD run
```bash
python FLD_ICC/train_FLD_ICC.py \
  -d physionet -ot 24 -bs 32 --epochs 1000 --early-stop 15 \
  -fn L -ed 64 -ld 128 -nh 4 --depth 2 \
  --lr 1e-4 --wd 1e-3 \
  --tbon --logdir runs/physionet_L_icfld
```

### Hyper-parameter searches
```bash
# Cross-dataset grid search (10 combinations per dataset/function)
python scripts/optuna_icfld.py --trials 10

# USHCN-only sweeps
python scripts/optuna_ushcn.py --trainer icfld --trials 10
python scripts/optuna_ushcn.py --trainer fld   --trials 10
```
Each sweep prints progress and the best configuration directly to stdout/stderr (no MLflow dependency).

### Exporting and replaying best configurations
```bash
# Aggregate the best configuration per dataset/function (requires MLflow logs)
python scripts/export_best_hparams.py \
    --mlflows-root mlflows \
    --output best_hparams.json

# Re-train every configuration recorded in the JSON (bash or PowerShell)
./scripts/run_best_hparams.sh best_hparams.json
pwsh ./scripts/run_best_hparams.ps1 -BestParamsPath best_hparams.json
```
The replay scripts automatically route USHCN runs through `train_FLD_ICC_ushcn.py`, enable TensorBoard logging, and store checkpoints in the appropriate `saved_models/` directory.

## Logging and artefacts
- MLflow runs are optional/legacy; if you still use them they live under `mlruns/` and can be exported with `scripts/export_best_hparams.py`.
- TensorBoard logs are collected in `runs/` (inspect with `tensorboard --logdir runs`).
- Model checkpoints are stored under `FLD_ICC/saved_models/` or `FLD/saved_models/` depending on the trainer.


---

<!-- Source: BENCHMARK_GUIDE.md -->

# FLD vs IC-FLD Benchmark Guide

Complete guide for running comprehensive benchmarks comparing FLD and IC-FLD on PhysioNet dataset.

---

## Overview

The benchmark scripts train both FLD and IC-FLD models on PhysioNet dataset across all four basis functions:
- **C** (Constant): 1 basis term
- **L** (Linear): 2 basis terms
- **Q** (Quadratic): 3 basis terms
- **S** (Sinusoidal): 4 basis terms (2 harmonics)

**Total runs:** 8 (4 functions √ó 2 models)
**Duration:** ~8-16 hours (depending on hardware) for 1000 epochs

---

## Quick Start

### Windows (PowerShell)

```powershell
# Run with default settings (1000 epochs)
.\scripts\benchmark_fld_vs_icfld_physionet.ps1

# Custom settings
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 `
  -Epochs 500 `
  -BatchSize 64 `
  -EarlyStop 20
```

### Linux/macOS (Bash)

```bash
# Make script executable
chmod +x scripts/benchmark_fld_vs_icfld_physionet.sh

# Run with default settings
./scripts/benchmark_fld_vs_icfld_physionet.sh

# Custom settings via environment variables
EPOCHS=500 BATCH_SIZE=64 EARLY_STOP=20 \
  ./scripts/benchmark_fld_vs_icfld_physionet.sh
```

---

## Configuration Options

### PowerShell Parameters

```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 `
  -Epochs 1000 `              # Number of epochs (default: 1000)
  -BatchSize 32 `             # Batch size (default: 32)
  -ObservationTime 24 `       # Observation window in hours (default: 24)
  -EarlyStop 15 `             # Early stopping patience (default: 15)
  -LearningRate 1e-4 `        # Learning rate (default: 1e-4)
  -WeightDecay 1e-3 `         # Weight decay (default: 1e-3)
  -Seed 42 `                  # Random seed (default: 42)
  -GPU "0"                    # GPU device ID (default: "0")
```

### Bash Environment Variables

```bash
EPOCHS=1000                # Number of epochs (default: 1000)
BATCH_SIZE=32              # Batch size (default: 32)
OBSERVATION_TIME=24        # Observation window in hours (default: 24)
EARLY_STOP=15              # Early stopping patience (default: 15)
LEARNING_RATE=1e-4         # Learning rate (default: 1e-4)
WEIGHT_DECAY=1e-3          # Weight decay (default: 1e-3)
SEED=42                    # Random seed (default: 42)
GPU=0                      # GPU device ID (default: 0)

./scripts/benchmark_fld_vs_icfld_physionet.sh
```

---

## Output Files

All results are saved in a timestamped directory: `benchmark_results_physionet_YYYYMMDD_HHMMSS/`

### Generated Files

```
benchmark_results_physionet_20251025_143022/
‚îú‚îÄ‚îÄ FLD_C.log                      # Training log for FLD with Constant basis
‚îú‚îÄ‚îÄ FLD_C_metrics.json             # Metrics JSON for FLD-C
‚îú‚îÄ‚îÄ FLD_L.log                      # Training log for FLD with Linear basis
‚îú‚îÄ‚îÄ FLD_L_metrics.json             # Metrics JSON for FLD-L
‚îú‚îÄ‚îÄ FLD_Q.log                      # Training log for FLD with Quadratic basis
‚îú‚îÄ‚îÄ FLD_Q_metrics.json             # Metrics JSON for FLD-Q
‚îú‚îÄ‚îÄ FLD_S.log                      # Training log for FLD with Sinusoidal basis
‚îú‚îÄ‚îÄ FLD_S_metrics.json             # Metrics JSON for FLD-S
‚îú‚îÄ‚îÄ ICFLD_C.log                    # Training log for IC-FLD with Constant basis
‚îú‚îÄ‚îÄ ICFLD_C_metrics.json           # Metrics JSON for IC-FLD-C
‚îú‚îÄ‚îÄ ICFLD_L.log                    # Training log for IC-FLD with Linear basis
‚îú‚îÄ‚îÄ ICFLD_L_metrics.json           # Metrics JSON for IC-FLD-L
‚îú‚îÄ‚îÄ ICFLD_Q.log                    # Training log for IC-FLD with Quadratic basis
‚îú‚îÄ‚îÄ ICFLD_Q_metrics.json           # Metrics JSON for IC-FLD-Q
‚îú‚îÄ‚îÄ ICFLD_S.log                    # Training log for IC-FLD with Sinusoidal basis
‚îú‚îÄ‚îÄ ICFLD_S_metrics.json           # Metrics JSON for IC-FLD-S
‚îú‚îÄ‚îÄ summary.csv                    # CSV summary of all results
‚îî‚îÄ‚îÄ summary.json                   # JSON summary (PowerShell only)
```

### Metrics JSON Format

Each `*_metrics.json` file contains:

```json
{
  "best_epoch": 87,
  "val_mse_best": 0.012345,
  "val_rmse_best": 0.111111,
  "val_mae_best": 0.089012,
  "test_mse_best": 0.013456,
  "test_rmse_best": 0.116012,
  "test_mae_best": 0.092345,
  "train_loss_last_batch": 0.011234,
  "total_time_seconds": 3456.78,
  "start_time": "2025-10-25 14:30:22",
  "end_time": "2025-10-25 15:28:19",
  "function": "L",
  "model_type": "ICFLD"
}
```

### Summary CSV Format

`summary.csv` contains all runs in tabular format:

```csv
Model,Function,BestEpoch,ValMSE,ValRMSE,ValMAE,TestMSE,TestRMSE,TestMAE,TotalTimeSeconds,TotalTimeMinutes
FLD,C,95,0.015234,0.123456,0.098765,0.016234,0.127456,0.102345,2345.67,39.09
FLD,L,87,0.012345,0.111111,0.089012,0.013456,0.116012,0.092345,2567.89,42.80
...
```

---

## Example Output

### Console Output

```
========================================
FLD vs IC-FLD Benchmark - PhysioNet
========================================
Configuration:
  Dataset:          physionet
  Epochs:           1000
  Batch Size:       32
  Observation Time: 24 hours
  Early Stop:       15 epochs
  Learning Rate:    1e-4
  Weight Decay:     1e-3
  Seed:             42
  GPU:              0
  Log Directory:    benchmark_results_physionet_20251025_143022

Functions to test: C, L, Q, S

========================================
Part 1: FLD Baseline
========================================

[1/8 - 12.5%] Running FLD with function 'C'...
----------------------------------------
Start time: 2025-10-25 14:30:22
...
End time:   2025-10-25 15:10:45
Duration:   2423 seconds (40.38 minutes)

Key Metrics:
  Best Epoch:       95
  Val MSE (best):   0.015234
  Val RMSE (best):  0.123456
  Val MAE (best):   0.098765
  Test MSE (best):  0.016234
  Test RMSE (best): 0.127456
  Test MAE (best):  0.102345
  Total Time:       2423 seconds

[2/8 - 25.0%] Running FLD with function 'L'...
...

========================================
BENCHMARK COMPLETE - SUMMARY REPORT
========================================

Timing Summary (PhysioNet, 1000 epochs):

Function   FLD (seconds)   ICFLD (seconds)  Speedup (FLD/ICFLD)
--------   --------------  ----------------  -------------------
C          2423            2156              1.12x
L          2567            2289              1.12x
Q          2678            2398              1.12x
S          2890            2567              1.13x
--------   --------------  ----------------  -------------------
TOTAL      10558           9410              1.12x

Performance Summary:

Function   Model    Val MSE         Test MSE        Val MAE         Test MAE
--------   -----    -------         --------        -------         --------
C          FLD      0.015234        0.016234        0.098765        0.102345
C          ICFLD    0.014123        0.015012        0.091234        0.094567
L          FLD      0.012345        0.013456        0.089012        0.092345
L          ICFLD    0.011234        0.012123        0.084567        0.087890
Q          FLD      0.011567        0.012678        0.085678        0.088901
Q          ICFLD    0.010456        0.011345        0.079012        0.082345
S          FLD      0.010890        0.011901        0.082345        0.085678
S          ICFLD    0.009678        0.010567        0.075678        0.078901

========================================
Files Generated:

Logs directory: benchmark_results_physionet_20251025_143022

  FLD_C.log
  FLD_C_metrics.json
  FLD_L.log
  FLD_L_metrics.json
  ...

========================================
CSV summary saved to: benchmark_results_physionet_20251025_143022/summary.csv

Benchmark completed successfully!
Total execution time: 332.80 minutes
```

---

## Analysis Workflow

### 1. View Summary in Excel/Google Sheets

Open `summary.csv` in Excel or Google Sheets:

```bash
# Windows
start excel.exe benchmark_results_physionet_*/summary.csv

# macOS
open -a "Microsoft Excel" benchmark_results_physionet_*/summary.csv

# Linux
libreoffice --calc benchmark_results_physionet_*/summary.csv
```

### 2. Compare Metrics Programmatically

```python
import pandas as pd
import matplotlib.pyplot as plt

# Load results
df = pd.read_csv('benchmark_results_physionet_*/summary.csv')

# Pivot for comparison
pivot = df.pivot_table(
    index='Function',
    columns='Model',
    values=['ValMSE', 'TestMSE', 'TotalTimeMinutes']
)

print(pivot)

# Plot comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Test MSE comparison
pivot['TestMSE'].plot(kind='bar', ax=axes[0], title='Test MSE by Function')
axes[0].set_ylabel('MSE')

# Training time comparison
pivot['TotalTimeMinutes'].plot(kind='bar', ax=axes[1], title='Training Time by Function')
axes[1].set_ylabel('Minutes')

# MSE improvement (FLD - ICFLD)
improvement = (pivot['TestMSE']['FLD'] - pivot['TestMSE']['ICFLD']) / pivot['TestMSE']['FLD'] * 100
improvement.plot(kind='bar', ax=axes[2], title='IC-FLD Improvement over FLD (%)')
axes[2].set_ylabel('Improvement (%)')
axes[2].axhline(y=0, color='r', linestyle='--')

plt.tight_layout()
plt.savefig('benchmark_comparison.png', dpi=300)
plt.show()
```

### 3. Extract Best Configuration

```python
import pandas as pd

df = pd.read_csv('benchmark_results_physionet_*/summary.csv')

# Find best IC-FLD configuration by test MSE
best_icfld = df[df['Model'] == 'ICFLD'].sort_values('TestMSE').iloc[0]
print("Best IC-FLD configuration:")
print(f"  Function: {best_icfld['Function']}")
print(f"  Test MSE: {best_icfld['TestMSE']:.6f}")
print(f"  Test MAE: {best_icfld['TestMAE']:.6f}")
print(f"  Best Epoch: {best_icfld['BestEpoch']}")
print(f"  Training Time: {best_icfld['TotalTimeMinutes']:.2f} minutes")

# Compare with FLD
best_fld = df[(df['Model'] == 'FLD') & (df['Function'] == best_icfld['Function'])].iloc[0]
improvement = (best_fld['TestMSE'] - best_icfld['TestMSE']) / best_fld['TestMSE'] * 100
print(f"\nImprovement over FLD: {improvement:.2f}%")
```

---

## Monitoring Progress

### Real-time Log Monitoring

**PowerShell:**
```powershell
# Monitor latest log file
Get-Content -Path "benchmark_results_physionet_*\*.log" -Tail 20 -Wait
```

**Bash:**
```bash
# Monitor latest log file
tail -f benchmark_results_physionet_*/FLD_L.log
```

### TensorBoard Monitoring

All runs log to TensorBoard. Monitor in real-time:

```bash
# Start TensorBoard
tensorboard --logdir runs

# Open browser to http://localhost:6006
```

Filter by run name pattern:
- FLD runs: `FLD_[CLQS]_physionet`
- IC-FLD runs: `ICFLD_[CLQS]_physionet`

---

## Resuming Interrupted Runs

If the benchmark is interrupted, you can manually resume individual runs:

### Resume FLD Training

```bash
python FLD/train_FLD.py \
  -d physionet \
  -ot 24 \
  -bs 32 \
  --epochs 1000 \
  --early-stop 15 \
  -fn L \
  -ld 20 \
  -ed 4 \
  -nh 4 \
  --depth 2 \
  --lr 1e-4 \
  --wd 1e-3 \
  --seed 42 \
  --resume auto  # Resume from latest checkpoint
```

### Resume IC-FLD Training

```bash
python FLD_ICC/train_FLD_ICC.py \
  -d physionet \
  -ot 24 \
  -bs 32 \
  --epochs 1000 \
  --early-stop 15 \
  -fn L \
  -ld 128 \
  -ed 64 \
  -nh 4 \
  --depth 2 \
  --lr 1e-4 \
  --wd 1e-3 \
  --seed 42 \
  --resume auto  # Resume from latest checkpoint
```

---

## Troubleshooting

### Issue: Script hangs or takes too long

**Solution 1:** Run with fewer epochs for initial testing:
```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

**Solution 2:** Test with a single function first by modifying the script:
```powershell
# Edit the script, change:
$Functions = @("C", "L", "Q", "S")
# to:
$Functions = @("L")  # Test only Linear basis
```

### Issue: Out of memory

**Solution:** Reduce batch size:
```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -BatchSize 16
```

### Issue: CUDA out of memory

**Solution:** Clear GPU memory between runs by adding to script:
```powershell
# Add after each training run:
Start-Sleep -Seconds 5  # Wait for GPU to clear
```

### Issue: Python crashes during training

**Solution:** Check individual log files for stack traces:
```bash
grep -i "error\|exception\|traceback" benchmark_results_*/FLD_L.log
```

---

## Hardware Recommendations

### Minimum Requirements
- **GPU:** NVIDIA GTX 1060 (6GB VRAM) or equivalent
- **RAM:** 16GB
- **Storage:** 10GB free space
- **Expected time:** ~16 hours for full benchmark (1000 epochs √ó 8 runs)

### Recommended Setup
- **GPU:** NVIDIA RTX 3080 (10GB VRAM) or better
- **RAM:** 32GB
- **Storage:** 20GB free space (for TensorBoard logs + checkpoints)
- **Expected time:** ~8 hours for full benchmark

### High-Performance Setup
- **GPU:** NVIDIA A100 (40GB VRAM)
- **RAM:** 64GB
- **Storage:** 50GB SSD
- **Expected time:** ~4-6 hours for full benchmark
- **Tip:** Use the optimized trainer for 2-3x speedup

---

## Best Practices

1. **Run overnight:** The full benchmark takes several hours
2. **Monitor GPU utilization:** Use `nvidia-smi -l 1` to ensure GPU is utilized
3. **Save logs:** Don't delete benchmark results until analysis is complete
4. **Use consistent seeds:** For reproducible comparisons across runs
5. **Backup checkpoints:** Model checkpoints are saved in `saved_models/`

---

## Citation

If you use these benchmark results in your research:

```bibtex
@inproceedings{icfld2025,
  title={Inter-Channel Functional Latent Dynamics for Irregular Time Series},
  author={[Your Name]},
  booktitle={[Conference/Journal]},
  year={2025}
}
```

---

## Support

For issues or questions:
1. Check individual log files for error messages
2. Review the troubleshooting section above
3. Verify dataset paths with `python scripts/check_data_paths.py`
4. Test single model runs before full benchmark

---

<!-- Source: BENCHMARK_QUICKSTART.md -->

# Benchmark Quick Start Guide

**One-command benchmarking for FLD vs IC-FLD on PhysioNet**

---

## TL;DR

### Windows
```powershell
# Full benchmark (1000 epochs, ~8-16 hours)
.\scripts\benchmark_fld_vs_icfld_physionet.ps1

# Quick test (100 epochs, ~1-2 hours)
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

### Linux/macOS
```bash
# Full benchmark (1000 epochs, ~8-16 hours)
chmod +x scripts/benchmark_fld_vs_icfld_physionet.sh
./scripts/benchmark_fld_vs_icfld_physionet.sh

# Quick test (100 epochs, ~1-2 hours)
EPOCHS=100 ./scripts/benchmark_fld_vs_icfld_physionet.sh
```

---

## What It Does

Trains **8 models total:**

| Model   | Functions Tested          | Latent Dim | Embed Dim | Heads | Depth |
|---------|---------------------------|------------|-----------|-------|-------|
| FLD     | C, L, Q, S (4 runs)       | 20         | 4         | 4     | 2     |
| IC-FLD  | C, L, Q, S (4 runs)       | 128        | 64        | 4     | 2     |

**Basis Functions:**
- **C** (Constant): 1 term
- **L** (Linear): 2 terms
- **Q** (Quadratic): 3 terms
- **S** (Sinusoidal): 4 terms

---

## Output Location

All results saved to: `benchmark_results_physionet_YYYYMMDD_HHMMSS/`

**Key files:**
- `summary.csv` - All metrics in spreadsheet format
- `FLD_L.log` - Training log for FLD with Linear basis
- `ICFLD_L_metrics.json` - Metrics JSON for IC-FLD with Linear basis

---

## Expected Results Format

### Console Summary
```
Timing Summary (PhysioNet, 1000 epochs):

Function   FLD (seconds)   ICFLD (seconds)  Speedup
--------   --------------  ----------------  --------
C          2423            2156              1.12x
L          2567            2289              1.12x
Q          2678            2398              1.12x
S          2890            2567              1.13x
--------   --------------  ----------------  --------
TOTAL      10558           9410              1.12x

Performance Summary:

Function   Model    Val MSE     Test MSE    Val MAE     Test MAE
--------   -----    -------     --------    -------     --------
C          FLD      0.015234    0.016234    0.098765    0.102345
C          ICFLD    0.014123    0.015012    0.091234    0.094567
L          FLD      0.012345    0.013456    0.089012    0.092345
L          ICFLD    0.011234    0.012123    0.084567    0.087890
...
```

### CSV Format (`summary.csv`)
```csv
Model,Function,BestEpoch,ValMSE,ValRMSE,ValMAE,TestMSE,TestRMSE,TestMAE,TotalTimeSeconds
FLD,C,95,0.015234,0.123456,0.098765,0.016234,0.127456,0.102345,2423
FLD,L,87,0.012345,0.111111,0.089012,0.013456,0.116012,0.092345,2567
ICFLD,C,82,0.014123,0.118765,0.091234,0.015012,0.122567,0.094567,2156
ICFLD,L,78,0.011234,0.105987,0.084567,0.012123,0.110123,0.087890,2289
...
```

---

## Quick Analysis

### 1. View Results in Excel
```bash
# Open CSV in Excel/LibreOffice
start excel benchmark_results_physionet_*/summary.csv
```

### 2. Find Best Model
```python
import pandas as pd
df = pd.read_csv('benchmark_results_physionet_*/summary.csv')

# Best IC-FLD by Test MSE
best = df[df['Model'] == 'ICFLD'].sort_values('TestMSE').iloc[0]
print(f"Best: {best['Function']} with Test MSE = {best['TestMSE']:.6f}")
```

### 3. Compare Models
```python
# IC-FLD improvement over FLD
for func in ['C', 'L', 'Q', 'S']:
    fld = df[(df['Model'] == 'FLD') & (df['Function'] == func)]['TestMSE'].values[0]
    icfld = df[(df['Model'] == 'ICFLD') & (df['Function'] == func)]['TestMSE'].values[0]
    improvement = (fld - icfld) / fld * 100
    print(f"{func}: {improvement:.2f}% improvement")
```

---

## Monitoring

### Real-Time Progress
```bash
# Watch latest log
tail -f benchmark_results_physionet_*/ICFLD_L.log

# Check GPU usage
watch -n 1 nvidia-smi
```

### TensorBoard
```bash
tensorboard --logdir runs
# Open: http://localhost:6006
```

---

## Common Customizations

### Faster Testing (100 epochs)
```powershell
# PowerShell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100

# Bash
EPOCHS=100 ./scripts/benchmark_fld_vs_icfld_physionet.sh
```

### Smaller Batch (if OOM)
```powershell
# PowerShell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -BatchSize 16

# Bash
BATCH_SIZE=16 ./scripts/benchmark_fld_vs_icfld_physionet.sh
```

### Different Observation Window
```powershell
# PowerShell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -ObservationTime 48

# Bash
OBSERVATION_TIME=48 ./scripts/benchmark_fld_vs_icfld_physionet.sh
```

---

## Estimated Timing

| Hardware              | 1000 Epochs (8 runs) | 100 Epochs (8 runs) |
|-----------------------|----------------------|---------------------|
| GTX 1060 (6GB)        | ~16 hours            | ~2 hours            |
| RTX 3080 (10GB)       | ~8 hours             | ~1 hour             |
| A100 (40GB)           | ~4-6 hours           | ~30-45 minutes      |
| A100 + Optimizations  | ~2-3 hours           | ~15-20 minutes      |

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| Script hangs | Reduce epochs: `-Epochs 100` |
| Out of memory | Reduce batch size: `-BatchSize 16` |
| CUDA error | Clear GPU: `nvidia-smi --gpu-reset` (requires sudo) |
| Missing dataset | Run: `python scripts/check_data_paths.py` |

---

## Next Steps

1. **Run quick test:** Start with 100 epochs to verify setup
2. **Monitor first run:** Watch logs and GPU utilization
3. **Run full benchmark:** Once verified, run with 1000 epochs
4. **Analyze results:** Use `summary.csv` for comparisons
5. **Report findings:** See `BENCHMARK_GUIDE.md` for analysis examples

---

## Full Documentation

- **`BENCHMARK_GUIDE.md`** - Complete guide with examples
- **`OPTIMIZATION_GUIDE.md`** - Speed up training 2-3x
- Project overview and architecture documentation

---

**Questions?** Check `BENCHMARK_GUIDE.md` for detailed troubleshooting.

---

<!-- Source: BENCHMARKING_COMPLETE.md -->

\# Benchmarking Scripts - Complete Setup

**Status:** ‚úÖ Ready to use

All benchmarking scripts and documentation have been created for comparing FLD vs IC-FLD on PhysioNet dataset.

---

## üì¶ What You Got

### Executable Scripts (2 files)
- ‚úÖ **`scripts/benchmark_fld_vs_icfld_physionet.ps1`** (PowerShell/Windows)
- ‚úÖ **`scripts/benchmark_fld_vs_icfld_physionet.sh`** (Bash/Linux/macOS) - executable

### Documentation (3 files)
- ‚úÖ **`BENCHMARK_QUICKSTART.md`** - Quick reference (start here!)
- ‚úÖ **`BENCHMARK_GUIDE.md`** - Complete guide with analysis examples
- ‚úÖ **`BENCHMARKING_COMPLETE.md`** - This file

---

## üöÄ Run Your First Benchmark

### Quick Test (100 epochs, ~1-2 hours)

**Windows:**
```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

**Linux/macOS:**
```bash
EPOCHS=100 ./scripts/benchmark_fld_vs_icfld_physionet.sh
```

### Full Benchmark (1000 epochs, ~8-16 hours)

**Windows:**
```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1
```

**Linux/macOS:**
```bash
./scripts/benchmark_fld_vs_icfld_physionet.sh
```

---

## üìä What Gets Tested

**8 Training Runs:**
1. FLD with Constant basis (C)
2. FLD with Linear basis (L)
3. FLD with Quadratic basis (Q)
4. FLD with Sinusoidal basis (S)
5. IC-FLD with Constant basis (C)
6. IC-FLD with Linear basis (L)
7. IC-FLD with Quadratic basis (Q)
8. IC-FLD with Sinusoidal basis (S)

**Dataset:** PhysioNet (24-hour observation window)

**Metrics Logged:**
- Best epoch
- Validation MSE, RMSE, MAE
- Test MSE, RMSE, MAE
- Training time (seconds and minutes)
- Start/end timestamps

---

## üìÅ Output Files

After running, you'll get a timestamped directory:
```
benchmark_results_physionet_20251025_143022/
‚îú‚îÄ‚îÄ FLD_C.log                 # Training logs
‚îú‚îÄ‚îÄ FLD_C_metrics.json        # Metrics JSON
‚îú‚îÄ‚îÄ FLD_L.log
‚îú‚îÄ‚îÄ FLD_L_metrics.json
‚îú‚îÄ‚îÄ FLD_Q.log
‚îú‚îÄ‚îÄ FLD_Q_metrics.json
‚îú‚îÄ‚îÄ FLD_S.log
‚îú‚îÄ‚îÄ FLD_S_metrics.json
‚îú‚îÄ‚îÄ ICFLD_C.log
‚îú‚îÄ‚îÄ ICFLD_C_metrics.json
‚îú‚îÄ‚îÄ ICFLD_L.log
‚îú‚îÄ‚îÄ ICFLD_L_metrics.json
‚îú‚îÄ‚îÄ ICFLD_Q.log
‚îú‚îÄ‚îÄ ICFLD_Q_metrics.json
‚îú‚îÄ‚îÄ ICFLD_S.log
‚îú‚îÄ‚îÄ ICFLD_S_metrics.json
‚îú‚îÄ‚îÄ summary.csv              # ‚≠ê Main results file
‚îî‚îÄ‚îÄ summary.json             # (PowerShell only)
```

---

## üìà Example Results

### Console Output at End

```
========================================
BENCHMARK COMPLETE - SUMMARY REPORT
========================================

Timing Summary (PhysioNet, 1000 epochs):

Function   FLD (seconds)   ICFLD (seconds)  Speedup (FLD/ICFLD)
--------   --------------  ----------------  -------------------
C          2423            2156              1.12x
L          2567            2289              1.12x
Q          2678            2398              1.12x
S          2890            2567              1.13x
--------   --------------  ----------------  -------------------
TOTAL      10558           9410              1.12x

Performance Summary:

Function   Model    Val MSE     Test MSE    Val MAE     Test MAE
--------   -----    -------     --------    -------     --------
C          FLD      0.015234    0.016234    0.098765    0.102345
C          ICFLD    0.014123    0.015012    0.091234    0.094567
L          FLD      0.012345    0.013456    0.089012    0.092345
L          ICFLD    0.011234    0.012123    0.084567    0.087890
Q          FLD      0.011567    0.012678    0.085678    0.088901
Q          ICFLD    0.010456    0.011345    0.079012    0.082345
S          FLD      0.010890    0.011901    0.082345    0.085678
S          ICFLD    0.009678    0.010567    0.075678    0.078901

========================================
Benchmark completed successfully!
Total execution time: 332.80 minutes
```

### CSV Summary (`summary.csv`)

Open in Excel/Sheets for analysis:

| Model  | Function | BestEpoch | ValMSE   | TestMSE  | ValMAE   | TestMAE  | TotalTimeSeconds | TotalTimeMinutes |
|--------|----------|-----------|----------|----------|----------|----------|------------------|------------------|
| FLD    | C        | 95        | 0.015234 | 0.016234 | 0.098765 | 0.102345 | 2423             | 40.38            |
| FLD    | L        | 87        | 0.012345 | 0.013456 | 0.089012 | 0.092345 | 2567             | 42.78            |
| ICFLD  | C        | 82        | 0.014123 | 0.015012 | 0.091234 | 0.094567 | 2156             | 35.93            |
| ICFLD  | L        | 78        | 0.011234 | 0.012123 | 0.084567 | 0.087890 | 2289             | 38.15            |
| ...    | ...      | ...       | ...      | ...      | ...      | ...      | ...              | ...              |

---

## üîç Monitoring Progress

### Real-Time Logs
```bash
# Watch current training
tail -f benchmark_results_physionet_*/ICFLD_L.log

# Check GPU usage
nvidia-smi -l 1  # Updates every second
```

### TensorBoard
```bash
tensorboard --logdir runs
# Open http://localhost:6006
```

Filter by pattern:
- FLD runs: `FLD_*_physionet`
- IC-FLD runs: `ICFLD_*_physionet`

---

## ‚öôÔ∏è Configuration Options

### PowerShell
```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 `
  -Epochs 1000 `          # Training epochs
  -BatchSize 32 `         # Batch size
  -ObservationTime 24 `   # Hours of observation
  -EarlyStop 15 `         # Early stopping patience
  -LearningRate 1e-4 `    # Learning rate
  -WeightDecay 1e-3 `     # L2 regularization
  -Seed 42 `              # Random seed
  -GPU "0"                # GPU device
```

### Bash
```bash
EPOCHS=1000 \
BATCH_SIZE=32 \
OBSERVATION_TIME=24 \
EARLY_STOP=15 \
LEARNING_RATE=1e-4 \
WEIGHT_DECAY=1e-3 \
SEED=42 \
GPU=0 \
./scripts/benchmark_fld_vs_icfld_physionet.sh
```

---

## üéØ Next Steps

1. **Verify setup:**
   ```bash
   python scripts/check_data_paths.py
   ```

2. **Quick test (100 epochs):**
   ```powershell
   .\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
   ```

3. **Monitor first run:**
   - Watch logs: `tail -f benchmark_results_*/FLD_C.log`
   - Check GPU: `nvidia-smi -l 1`

4. **Run full benchmark (1000 epochs):**
   ```powershell
   .\scripts\benchmark_fld_vs_icfld_physionet.ps1
   ```

5. **Analyze results:**
   - Open `summary.csv` in Excel
   - See `BENCHMARK_GUIDE.md` for analysis examples

---

## üìö Documentation Reference

| File | Purpose |
|------|---------|
| **BENCHMARK_QUICKSTART.md** | Quick reference (TL;DR) |
| **BENCHMARK_GUIDE.md** | Complete guide with analysis examples |
| **OPTIMIZATION_GUIDE.md** | Speed up training 2-3x |
| Project overview | repository documentation |

---

## ‚è±Ô∏è Expected Timing

| Hardware | 1000 Epochs (8 runs) | 100 Epochs (8 runs) |
|----------|----------------------|---------------------|
| GTX 1060 | ~16 hours            | ~2 hours            |
| RTX 3080 | ~8 hours             | ~1 hour             |
| A100     | ~4-6 hours           | ~30-45 minutes      |

**Tip:** Use `train_FLD_ICC_optimized.py` for 2-3x speedup!

---

## üõ†Ô∏è Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| Script hangs | Start with `-Epochs 100` to test |
| Out of memory | Use `-BatchSize 16` |
| CUDA error | Check GPU availability: `nvidia-smi` |
| Missing dataset | Run: `python scripts/check_data_paths.py` |
| Import errors | Check: `pip install -r requirements.txt` |

### Check Logs
```bash
# Find errors in logs
grep -i "error\|exception" benchmark_results_*/FLD_L.log
```

---

## ‚úÖ Checklist Before Running

- [ ] Datasets accessible (`python scripts/check_data_paths.py`)
- [ ] GPU available (`nvidia-smi`)
- [ ] Sufficient disk space (10GB+)
- [ ] Environment activated (`.venv/Scripts/activate` or `source .venv/bin/activate`)
- [ ] Dependencies installed (`pip install -r requirements.txt`)

---

## üéâ Quick Win Example

**Run 100-epoch test to verify setup:**

```powershell
# Windows - should complete in 1-2 hours
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100

# Expected output:
# ========================================
# FLD vs IC-FLD Benchmark - PhysioNet
# ========================================
# ...
# [1/8 - 12.5%] Running FLD with function 'C'...
# ...
# Benchmark completed successfully!
```

---

## üìû Support

1. **Quick questions:** Check `BENCHMARK_QUICKSTART.md`
2. **Detailed guide:** See `BENCHMARK_GUIDE.md`
3. **Analysis examples:** `BENCHMARK_GUIDE.md` Section "Analysis Workflow"
4. **Performance tuning:** See `OPTIMIZATION_GUIDE.md`

---

## üö¶ Ready to Start!

Everything is set up and ready to go. Run your first quick test:

```powershell
# PowerShell (Windows)
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100

# Bash (Linux/macOS)
EPOCHS=100 ./scripts/benchmark_fld_vs_icfld_physionet.sh
```

**Good luck with your benchmarking! üéØ**

---

<!-- Source: IC-FLD_Channel_Processing_Analysis.md -->

# IC-FLD Channel Processing Analysis

**Date:** 2025-10-25
**Purpose:** Document how channels are processed in the attention mechanism of IC-FLD vs FLD

---

## Executive Summary

**Yes, IC-FLD sends all channels together through the attention layer** by flattening the entire (time √ó channel) history into a single sequence. This is the fundamental architectural difference from the original FLD model.

- **IC-FLD**: Flattens `[B, T, C]` ‚Üí `[B, T√óC, E]` before attention
- **FLD**: Processes `[B, T, E]` with channel information encoded in the value dimension

---

## Detailed Analysis

### IC-FLD Architecture (`FLD_ICC/FLD_ICC.py`)

#### 1. Memory Construction (Lines 163-171)

```python
# Time embeddings: broadcast to all channels
Et = self._time_embed(timesteps).unsqueeze(2).expand(-1, -1, C, -1)  # [B,T,C,E]

# Channel embeddings: broadcast to all time steps
Ec = self.channel_embed.unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)  # [B,T,C,E]

# Value projection: per-observation embedding
Ex = self.value_proj(X.unsqueeze(-1))  # [B,T,C,E]

# Combine all three components
KV = Et + Ec + Ex  # [B,T,C,E]
```

**Key Insight:** Each observation is embedded using three components:
- **Time position**: When was this measurement taken?
- **Channel identity**: Which variable is this?
- **Observed value**: What was measured?

#### 2. Flattening Operation (Line 169)

```python
K = KV.view(B, T*C, self.E)  # [B, S, E] where S = T√óC
```

**Critical Step:** The 4D tensor `[B,T,C,E]` is reshaped to `[B, T√óC, E]`, creating a **single unified sequence** containing all observations from all channels across all time steps.

**Example:**
- Input: 3 time steps, 5 channels ‚Üí 15 total observations
- These 15 observations form one sequence for attention
- If some observations are missing (mask=0), they're still in the sequence but masked out

#### 3. Mask Flattening (Line 171)

```python
mask_flat = M.reshape(B, T*C).bool()  # [B, S]
```

The mask follows the same flattening pattern, ensuring missing observations don't contribute to attention.

#### 4. Query Structure (Line 174)

```python
Q = self.q_basis.unsqueeze(0).expand(B, -1, -1)  # [B, P, E]
```

**Per-basis queries**: The model learns `P` query vectors (one per basis function), shared across:
- All batch samples
- All channels
- All time steps

These queries ask: "What are the coefficient values for this basis function?"

#### 5. Cross-Attention (Line 177)

```python
Z = self.attn(Q, K, V, mask_flat)  # [B, P, latent_dim]
```

**Attention mechanism:**
- **Queries**: `[B, P, E]` - one per basis function
- **Keys/Values**: `[B, T√óC, E]` - flattened observation sequence
- **Output**: `[B, P, latent_dim]` - latent representation per basis

**What happens:**
1. Each basis query attends to **all** `T√óC` observations simultaneously
2. The attention weights decide which observations (from which channels, at which times) are relevant for that basis
3. This allows **cross-channel dependencies**: the coefficient for one channel can depend on observations from other channels

#### 6. Channel Projection (Line 180)

```python
coeffs = self.coeff_proj(Z)  # [B, P, C]
```

**Final step**: Project each basis's latent representation to per-channel coefficients.
- Input: `[B, P, latent_dim]` - shared representation
- Output: `[B, P, C]` - separate coefficient for each channel

**Result:** Each channel gets its own coefficient for each basis, but these coefficients are computed by attending over **all channels' histories**.

#### 7. Prediction (Lines 183-184)

```python
Phi = self._basis(y_times)         # [B, Ty, P]
Y = torch.einsum("btp,bpc->btc", Phi, coeffs)  # [B, Ty, C]
```

Standard functional latent dynamics: combine basis functions with learned coefficients.

---

### FLD Architecture (`FLD/FLD.py`) - For Comparison

#### 1. Input Processing (Lines 162-164)

```python
key = self.learn_time_embedding(timesteps)  # [B, L, E]
Xcat = torch.cat((X, M), -1)               # [B, L, 2D]
Mcat = torch.cat((M, M), -1)               # [B, L, 2D]
```

**Key difference:** No channel dimension in the time embeddings. Channel information is encoded in the **value** dimension (concatenating X and M along the feature axis).

#### 2. Attention (Line 168)

```python
coeffs = self.attn(query, key, Xcat, Mcat)  # [B, P, latent_dim]
```

**FLDAttention internals (Lines 49-55):**
```python
scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)  # [B,H,P,T]
scores = scores.unsqueeze(-1).repeat_interleave(dim, dim=-1)          # [B,H,P,T,D]
# ... masking ...
p_attn = F.softmax(scores, dim=-2)  # over T
return torch.sum(p_attn * value.unsqueeze(1).unsqueeze(1), -2), p_attn
```

**Critical observation:**
- Attention scores are computed over **time dimension only** `[B,H,P,T]`
- These scores are then broadcast to channels `[B,H,P,T,D]`
- Each channel uses the **same attention weights** but different values
- This is effectively **per-channel processing** with shared temporal attention patterns

#### 3. Output (Line 129)

```python
self.out = nn.Sequential(*decoder)  # latent_dim ‚Üí input_dim
```

The decoder directly maps from latent representation to channel predictions.

---

## Key Architectural Differences

| Aspect | IC-FLD | FLD |
|--------|--------|-----|
| **Memory Sequence** | `[B, T√óC, E]` - flattened | `[B, T, 2D]` - time only |
| **Channel Encoding** | Learned embeddings `[C, E]` | Implicit in value dimension |
| **Attention Scope** | All observations (time √ó channel) | Time steps only |
| **Cross-Channel** | Yes - explicit | Limited - same attention weights |
| **Coefficient Generation** | Shared latent ‚Üí per-channel projection | Direct per-channel output |
| **Parameters** | Channel embeddings + projection layer | No explicit channel parameters |

---

## Implications

### 1. **Inter-Channel Dependencies**
IC-FLD can model **explicit cross-channel relationships**:
- Example: Heart rate predictions can attend to blood pressure observations
- The attention mechanism learns which channels are relevant for each basis

### 2. **Computational Complexity**
- **IC-FLD**: Attention over `T√óC` sequence (quadratic in total observations)
- **FLD**: Attention over `T` sequence (quadratic in time steps only)
- IC-FLD is more expensive when `C` is large

### 3. **Representational Power**
IC-FLD's unified memory allows:
- Different time steps from different channels to interact
- Learned channel identities (via `channel_embed`)
- More flexible coefficient computation

### 4. **Missing Data Handling**
Both models handle missingness, but differently:
- **IC-FLD**: Each missing observation is a masked slot in the flattened sequence
- **FLD**: Missing channels affect the concatenated value dimension

---

## Code Trace: Full Forward Pass Example

### IC-FLD with 3 time steps, 2 channels, 2 basis functions

**Input:**
```python
timesteps = [0.0, 0.5, 1.0]  # [B=1, T=3]
X = [[1.2, 0.8],             # [B=1, T=3, C=2]
     [1.5, 0.0],
     [1.8, 0.9]]
M = [[1, 1],                 # [B=1, T=3, C=2]
     [1, 0],
     [1, 1]]
```

**Step 1: Embed observations**
```python
# Each (time, channel, value) tuple gets embedded:
Et[0,0,0,:] = time_embed(0.0)   # time=0, channel=0
Et[0,0,1,:] = time_embed(0.0)   # time=0, channel=1
...
Ec[0,:,0,:] = channel_embed[0]  # channel=0 at all times
Ec[0,:,1,:] = channel_embed[1]  # channel=1 at all times
...
Ex[0,0,0,:] = value_proj(1.2)   # observed value
Ex[0,1,1,:] = value_proj(0.0)   # missing (will be masked)
```

**Step 2: Flatten**
```python
K = [t0c0, t0c1, t1c0, t1c1, t2c0, t2c1]  # [1, 6, E]
mask_flat = [True, True, True, False, True, True]  # [1, 6]
```

**Step 3: Attend**
```python
Q = [q_basis_0, q_basis_1]  # [1, 2, E]

# q_basis_0 attends to all 6 observations:
attention_weights[0] = softmax([w_t0c0, w_t0c1, w_t1c0, -inf, w_t2c0, w_t2c1])
# Missing t1c1 gets -inf ‚Üí 0 after softmax

# Similarly for q_basis_1
```

**Step 4: Project to channels**
```python
Z = [z_basis_0, z_basis_1]  # [1, 2, latent_dim]
coeffs = [[c00, c01],       # [1, 2, 2]
          [c10, c11]]       # basis √ó channel
# c00 = coefficient for basis_0, channel_0
# c01 = coefficient for basis_0, channel_1
# etc.
```

**Step 5: Predict**
```python
# For linear basis (L):
Phi(t) = [1, t]  # [B, Ty, 2]
Y = Phi @ coeffs  # [B, Ty, 2]
# channel_0 = c00 * 1 + c10 * t
# channel_1 = c01 * 1 + c11 * t
```

---

## Conclusion

**IC-FLD fundamentally processes all channels together** by:
1. Creating a unified embedding space with time, channel, and value information
2. Flattening `[B, T, C, E]` ‚Üí `[B, T√óC, E]` to form a single sequence
3. Applying multi-head attention over this joint (time √ó channel) sequence
4. Projecting shared latent representations to per-channel coefficients

This architecture enables **cross-channel attention**, allowing the model to learn which observations from which channels are relevant for predicting each channel's functional coefficients.

The original FLD processes time only, with channel information implicitly encoded in the value dimension, resulting in shared temporal attention patterns across channels but no explicit cross-channel reasoning.

---

## References

- **IC-FLD Implementation**: `FLD_ICC/FLD_ICC.py:48-192`
- **FLD Implementation**: `FLD/FLD.py:82-192`
- **Key Lines**:
  - IC-FLD flattening: `FLD_ICC/FLD_ICC.py:169`
  - IC-FLD channel embeddings: `FLD_ICC/FLD_ICC.py:92`
  - IC-FLD attention call: `FLD_ICC/FLD_ICC.py:177`
  - FLD attention mechanism: `FLD/FLD.py:40-55`

---

<!-- Source: LOGGING_FIXES.md -->

# Logging Fixes - Real-Time Output & Metrics Extraction

## Issues Fixed

### 1. **No Real-Time Output in Benchmark Scripts**

**Problem:** When running benchmark scripts, you couldn't see training progress in real-time. Logs only appeared after completion.

**Cause:** Python buffers stdout/stderr by default when output is piped.

**Fix:** Added `-u` (unbuffered) flag to Python calls in benchmark scripts.

**Files Modified:**
- `scripts/benchmark_fld_vs_icfld_physionet.ps1`
- `scripts/benchmark_fld_vs_icfld_physionet.sh`
- `scripts/test_fld_quick.ps1`

**Before:**
```powershell
python FLD/train_FLD.py ... 2>&1 | Tee-Object -FilePath $LogFile
# No output until script finishes!
```

**After:**
```powershell
python -u FLD/train_FLD.py ... 2>&1 | Tee-Object -FilePath $LogFile
# Real-time output! Epoch logs appear as they happen
```

---

### 2. **FLD Metrics Not Extracted**

**Problem:** Benchmark scripts showed "Warning: Could not extract metrics from output" for FLD.

**Cause:** FLD trainer didn't output JSON at the end (unlike IC-FLD).

**Fix:** Added JSON output to FLD trainer (same format as IC-FLD).

**File Modified:**
- `FLD/train_FLD.py` (lines 292-306)

**Added Code:**
```python
# ---- Output JSON for benchmark scripts ----
import json
json_summary = {}
for key, value in metrics.items():
    if value is None:
        continue
    try:
        numeric = float(value)
    except (TypeError, ValueError):
        continue
    if isinstance(numeric, float) and (numeric != numeric or abs(numeric) == float("inf")):
        continue  # Skip NaN and inf
    json_summary[key] = numeric

print(json.dumps(json_summary))
```

**Now FLD outputs:**
```json
{"best_epoch": 67, "val_mse_best": 0.012345, "val_rmse_best": 0.111111, "val_mae_best": 0.089012, "train_loss_last_batch": 0.011234, "test_mse_best": 0.013456, "test_rmse_best": 0.116012, "test_mae_best": 0.092345}
```

---

## Expected Behavior Now

### Running FLD Directly (Real-Time Output)

```bash
python -u FLD/train_FLD.py -d physionet -e 5 -fn L ...
```

**You should see (in real-time):**
```
Dataset n_samples: 8000 6000 1200 800
Test record ids (first 20): [...]
...
data_min: tensor([...], device='cuda:0')
data_max: tensor([...], device='cuda:0')
time_max: tensor(48., device='cuda:0')
PID, device: 18816 cuda
Dataset=physionet, INPUT_DIM=41, history=24
n_train_batches: 225

- Epoch 001 | train_loss(one-batch): 0.028510 | val_loss: 0.028467 | ...
- Epoch 002 | train_loss(one-batch): 0.010109 | val_loss: 0.011775 | ...
- Epoch 003 | train_loss(one-batch): 0.009234 | val_loss: 0.010456 | ...
...

Best val MSE: 0.010456 @ epoch 3
Saved best:   saved_models/FLD-L_physionet_6376596.best.pt
Saved latest: saved_models/FLD-L_physionet_6376596.latest.pt
Test metrics ‚Äî Loss: 0.010234, MSE: 0.010234, RMSE: 0.101164, MAE: 0.065432

{"best_epoch": 3, "val_mse_best": 0.010456, "val_rmse_best": 0.102244, "val_mae_best": 0.066789, "train_loss_last_batch": 0.009234, "test_mse_best": 0.010234, "test_rmse_best": 0.101164, "test_mae_best": 0.065432}
```

---

### Running Benchmark Script (Real-Time Output + Metrics)

```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

**You should see (in real-time):**
```
========================================
FLD vs IC-FLD Benchmark - PhysioNet
========================================
Configuration:
  Dataset:          physionet
  Epochs:           100
  ...

========================================
Part 1: FLD Baseline
========================================

[1/8 - 12.5%] Running FLD with function 'C'...
----------------------------------------
Start time: 2025-10-25 14:30:22

Dataset n_samples: 8000 6000 1200 800
...
PID, device: 18816 cuda
Dataset=physionet, INPUT_DIM=41, history=24
n_train_batches: 225

- Epoch 001 | train_loss(one-batch): 0.028510 | val_loss: 0.028467 | ...
- Epoch 002 | train_loss(one-batch): 0.010109 | val_loss: 0.011775 | ...
- Epoch 003 | train_loss(one-batch): 0.009234 | val_loss: 0.010456 | ...
...
- Epoch 100 | train_loss(one-batch): 0.005678 | val_loss: 0.006234 | ...

Best val MSE: 0.006234 @ epoch 87
...
End time:   2025-10-25 14:45:18
Duration:   896 seconds (14.93 minutes)

Key Metrics:
  Best Epoch:       87
  Val MSE (best):   0.006234
  Val RMSE (best):  0.078955
  Val MAE (best):   0.052345
  Test MSE (best):  0.006456
  Test RMSE (best): 0.080349
  Test MAE (best):  0.053678
  Total Time:       896 seconds

[2/8 - 25.0%] Running FLD with function 'L'...
----------------------------------------
...
```

---

## Testing

### Quick Test (5 epochs)

```powershell
.\scripts\test_fld_quick.ps1
```

**Expected:**
- ‚úÖ See dataset loading messages immediately
- ‚úÖ See PID and device info
- ‚úÖ See epoch progress in real-time (not all at once at the end!)
- ‚úÖ See "Best val MSE" message
- ‚úÖ See JSON output line at the very end
- ‚úÖ Takes ~30-60 seconds (not 4 seconds!)

**Example Output:**
```
Testing FLD training script...

Dataset n_samples: 8000 6000 1200 800
...
- Epoch 001 | train_loss(one-batch): 0.028510 | ...
- Epoch 002 | train_loss(one-batch): 0.010109 | ...
...
{"best_epoch": 5, "val_mse_best": 0.009123, ...}

========================================
Test completed!
Duration: 45.67 seconds
SUCCESS: FLD training ran successfully!
```

---

## What You Should See vs What You Shouldn't

### ‚úÖ CORRECT (Real-Time Logs)

You should see logs **appearing progressively**:
```
[10:30:22] Dataset loading...
[10:30:25] PID, device: 18816 cuda
[10:30:26] - Epoch 001 | ...
[10:30:35] - Epoch 002 | ...
[10:30:44] - Epoch 003 | ...  ‚Üê Logs appear as training runs
...
```

### ‚ùå INCORRECT (Buffered/No Logs)

You should NOT see:
```
[Starting training...]
[10 minutes of silence...]
[10:40:22] Everything dumps at once:
Dataset loading...
Epoch 001 | ...
Epoch 002 | ...
...
Epoch 100 | ...
```

---

## Troubleshooting

### Issue: Still no real-time output

**Check 1:** Verify `-u` flag is being used:
```powershell
# Should see "-u" in the command
Get-Content scripts/benchmark_fld_vs_icfld_physionet.ps1 | Select-String "python -u"
```

**Check 2:** Try running FLD directly with `-u`:
```powershell
python -u FLD/train_FLD.py -d physionet -e 2 -fn L -ed 4 -nh 4 -dp 2
```

**Check 3:** Windows Terminal might buffer output. Try PowerShell ISE or Windows Terminal with different settings.

---

### Issue: Metrics still not extracted

**Check 1:** Verify JSON is output at the end:
```bash
python -u FLD/train_FLD.py -d physionet -e 2 -fn L ... | tail -1
```

Should output something like:
```json
{"best_epoch": 2, "val_mse_best": 0.012345, ...}
```

**Check 2:** Check log files:
```bash
tail -20 benchmark_results_*/FLD_L.log
```

Last line should be valid JSON.

---

### Issue: Output appears but very slowly

**Possible causes:**
1. Dataset loading takes time (normal on first run)
2. CUDA initialization (normal, happens once)
3. Batch size too large for GPU (reduce with `-bs 16`)

**Not a problem if:**
- You see "Dataset n_samples" within 30 seconds
- First epoch starts within 1 minute
- Each epoch takes reasonable time (30s-2min depending on hardware)

---

## Summary of Changes

| File | Change | Purpose |
|------|--------|---------|
| `FLD/train_FLD.py:292-306` | Added JSON output | Enable metrics extraction |
| `scripts/benchmark_fld_vs_icfld_physionet.ps1:122` | Added `python -u` | Real-time output (PowerShell) |
| `scripts/benchmark_fld_vs_icfld_physionet.sh:122` | Added `python -u` | Real-time output (Bash) |
| `scripts/test_fld_quick.ps1:9` | Added `python -u` | Real-time output (test script) |

---

## Verification

Run the quick test and verify:

```powershell
.\scripts\test_fld_quick.ps1
```

**Checklist:**
- [ ] Logs appear immediately (not after 30+ seconds)
- [ ] Epoch progress shows in real-time
- [ ] JSON line appears at the end
- [ ] "SUCCESS: FLD training ran successfully!" message
- [ ] Duration is reasonable (30-60s for 5 epochs)

**If all checked:** You're ready to run the full benchmark! üéâ

```powershell
# Quick benchmark (100 epochs, ~1-2 hours)
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100

# Full benchmark (1000 epochs, ~8-16 hours)
.\scripts\benchmark_fld_vs_icfld_physionet.ps1
```

---

<!-- Source: OPTIMIZATION_GUIDE.md -->

# IC-FLD Performance Optimization Guide

**Date:** 2025-10-25
**Purpose:** Pythonic and PyTorch-specific optimizations for faster IC-FLD training and inference

---

## Table of Contents

1. [Critical Bottlenecks Identified](#critical-bottlenecks-identified)
2. [Model Architecture Optimizations](#model-architecture-optimizations)
3. [Training Loop Optimizations](#training-loop-optimizations)
4. [Data Loading Optimizations](#data-loading-optimizations)
5. [Memory Optimizations](#memory-optimizations)
6. [Inference Optimizations](#inference-optimizations)
7. [Quick Wins Summary](#quick-wins-summary)

---

## Critical Bottlenecks Identified

### 1. **Embedding Construction** (`FLD_ICC.py:163-167`)
**Issue:** Multiple `expand()` operations create redundant memory views.

**Current Code:**
```python
Et = self._time_embed(timesteps).unsqueeze(2).expand(-1, -1, C, -1)  # [B,T,C,E]
Ec = self.channel_embed.unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)  # [B,T,C,E]
Ex = self.value_proj(X.unsqueeze(-1))  # [B,T,C,E]
KV = Et + Ec + Ex
```

**Impact:**
- Creates large intermediate tensors
- Multiple broadcasting operations
- Memory allocation overhead

### 2. **Attention Over T√óC Sequence** (`FLD_ICC.py:177`)
**Issue:** Quadratic complexity in `T√óC` grows quickly with many channels.

**Scaling:**
- PhysioNet: T=24, C=37 ‚Üí S=888 (manageable)
- MIMIC: T=48, C=96 ‚Üí S=4,608 (expensive)
- Attention is O(S¬≤) = O((T√óC)¬≤)

### 3. **Sequential Batch Processing** (`train_FLD_ICC.py:586-594`)
**Issue:** No batch accumulation or mixed precision training.

### 4. **Evaluation Loop** (`train_FLD_ICC.py:563-580`)
**Issue:** Synchronous processing with no optimization.

---

## Model Architecture Optimizations

### Optimization 1: Pre-compute Channel Embeddings

**Problem:** `channel_embed` is expanded every forward pass (`FLD_ICC.py:165`).

**Solution:** Cache batch-size-independent tensors.

**Before:**
```python
def forward(self, timesteps, X, M, y_times, denorm_time_max=None):
    B, T, C = X.shape
    # ...
    Ec = self.channel_embed.unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)  # [B,T,C,E]
```

**After:**
```python
def __init__(self, ...):
    # ... existing init code ...
    # Pre-expand channel embeddings (no batch/time dependency)
    self.register_buffer('_channel_embed_expanded', None)

def forward(self, timesteps, X, M, y_times, denorm_time_max=None):
    B, T, C = X.shape

    # Lazy initialization on first forward pass
    if self._channel_embed_expanded is None or self._channel_embed_expanded.shape[0] != T:
        # [C, E] -> [1, T, C, E]
        self._channel_embed_expanded = self.channel_embed.unsqueeze(0).unsqueeze(0).expand(1, T, -1, -1)

    # Just expand batch dimension (much cheaper)
    Ec = self._channel_embed_expanded.expand(B, -1, -1, -1)  # [B,T,C,E]
```

**Expected Speedup:** 5-10% reduction in forward pass time.

---

### Optimization 2: Fused Embedding Construction

**Problem:** Three separate operations + addition (`FLD_ICC.py:164-167`).

**Solution:** Use `torch.addcmul` or pre-allocate output tensor.

**Before:**
```python
Et = self._time_embed(timesteps).unsqueeze(2).expand(-1, -1, C, -1)
Ec = self.channel_embed.unsqueeze(0).unsqueeze(0).expand(B, T, -1, -1)
Ex = self.value_proj(X.unsqueeze(-1))
KV = Et + Ec + Ex
```

**After:**
```python
# Pre-allocate output (avoids intermediate allocations)
KV = torch.empty(B, T, C, self.E, device=X.device, dtype=X.dtype)

# Time embeddings [B,T,E] -> [B,T,1,E] -> add to KV
time_emb = self._time_embed(timesteps).unsqueeze(2)  # [B,T,1,E]

# Channel embeddings [C,E] -> [1,1,C,E]
chan_emb = self.channel_embed.unsqueeze(0).unsqueeze(0)  # [1,1,C,E]

# Value projection [B,T,C,E]
val_emb = self.value_proj(X.unsqueeze(-1))

# Fused addition with broadcasting (no intermediate tensors)
torch.add(time_emb, chan_emb, out=KV)  # KV = time + channel
KV.add_(val_emb)  # KV += values (in-place)
```

**Expected Speedup:** 10-15% reduction in memory allocations.

---

### Optimization 3: Efficient Masking

**Problem:** Mask flattening creates a copy (`FLD_ICC.py:171`).

**Before:**
```python
mask_flat = M.reshape(B, T*C).bool()  # Creates copy if not contiguous
```

**After:**
```python
# Ensure contiguity before reshape (avoids hidden copies)
mask_flat = M.contiguous().view(B, T*C).bool()
```

**Expected Speedup:** 2-5% (especially for large batches).

---

### Optimization 4: Flash Attention (PyTorch 2.0+)

**Problem:** Standard attention is O(S¬≤) in memory and compute.

**Solution:** Use `torch.nn.functional.scaled_dot_product_attention` (SDPA).

**Before (FLDAttention):**
```python
def forward(self, Q, K, V, mask):
    # ... projections ...
    scores = torch.einsum("bhpk,bhsk->bhps", Qp, Kp) / math.sqrt(self.k)
    m = mask.unsqueeze(1).unsqueeze(2)  # [B,1,1,S]
    scores = scores.masked_fill(~m, float("-inf"))
    A = F.softmax(scores, dim=-1)
    C = torch.einsum("bhps,bhsk->bhpk", A, Vp)
```

**After:**
```python
def forward(self, Q, K, V, mask):
    B, P, E = Q.shape
    S = K.shape[1]

    Qp = self.Wq(Q).view(B, P, self.h, self.k).transpose(1, 2)  # [B,h,P,k]
    Kp = self.Wk(K).view(B, S, self.h, self.k).transpose(1, 2)  # [B,h,S,k]
    Vp = self.Wv(V).view(B, S, self.h, self.k).transpose(1, 2)  # [B,h,S,k]

    # Convert mask: [B,S] -> [B,1,1,S] (broadcast to [B,h,P,S])
    attn_mask = mask.unsqueeze(1).unsqueeze(2).expand(-1, self.h, P, -1)

    # Flash Attention (memory-efficient, fused kernel)
    C = F.scaled_dot_product_attention(
        Qp, Kp, Vp,
        attn_mask=attn_mask,
        dropout_p=0.0,
        is_causal=False
    )  # [B,h,P,k]

    C = C.transpose(1, 2).contiguous().view(B, P, self.E)
    return self.Wo(C)
```

**Expected Speedup:** 20-40% faster attention (especially on A100/H100 GPUs).

**Requirements:**
```python
import torch
assert torch.__version__ >= "2.0.0"
```

---

## Training Loop Optimizations

### Optimization 5: Automatic Mixed Precision (AMP)

**Problem:** Full fp32 training is 2-3x slower than necessary.

**Solution:** Use PyTorch AMP with gradient scaling.

**Before (`train_FLD_ICC.py:586-594`):**
```python
for _ in range(num_train_batches):
    optimizer.zero_grad(set_to_none=True)
    batch = utils.get_next_batch(data_obj["train_dataloader"])
    T, X, M, TY, Y, YM = batch_to_icfld(batch, INPUT_DIM, DEVICE)
    YH = MODEL(T, X, M, TY, denorm_time_max=...)
    loss = mse_masked(Y, YH, YM)
    loss.backward()
    optimizer.step()
```

**After:**
```python
from torch.cuda.amp import autocast, GradScaler

# Add to setup section (line ~480)
scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))

# Modified training loop
for _ in range(num_train_batches):
    optimizer.zero_grad(set_to_none=True)
    batch = utils.get_next_batch(data_obj["train_dataloader"])
    T, X, M, TY, Y, YM = batch_to_icfld(batch, INPUT_DIM, DEVICE)

    with autocast(device_type=DEVICE.type, dtype=torch.float16):
        YH = MODEL(T, X, M, TY, denorm_time_max=...)
        loss = mse_masked(Y, YH, YM)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**Expected Speedup:** 1.5-2x faster training on CUDA, ~40% memory reduction.

**Compatibility:** Works on any NVIDIA GPU (Volta+), even without Tensor Cores.

---

### Optimization 6: Gradient Accumulation

**Problem:** Small batch sizes limit GPU utilization.

**Solution:** Accumulate gradients over multiple micro-batches.

**Implementation:**
```python
# Add to argparse (line ~326)
p.add_argument("--grad-accum-steps", type=int, default=1,
               help="Number of gradient accumulation steps (effective_bs = bs * accum_steps)")

# Modified training loop
accumulation_steps = args.grad_accum_steps
for batch_idx in range(num_train_batches):
    batch = utils.get_next_batch(data_obj["train_dataloader"])
    T, X, M, TY, Y, YM = batch_to_icfld(batch, INPUT_DIM, DEVICE)

    with autocast(device_type=DEVICE.type, dtype=torch.float16):
        YH = MODEL(T, X, M, TY, denorm_time_max=...)
        loss = mse_masked(Y, YH, YM) / accumulation_steps  # Scale loss

    scaler.scale(loss).backward()

    if (batch_idx + 1) % accumulation_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad(set_to_none=True)
```

**Example:** With `--grad-accum-steps 4`, effective batch size = 32 √ó 4 = 128.

**Expected Speedup:** Better GPU utilization (especially with small batches).

---

### Optimization 7: Compiled Model (PyTorch 2.0+)

**Problem:** Python overhead in repeated forward/backward passes.

**Solution:** Use `torch.compile()` for JIT optimization.

**Implementation (add after line 448):**
```python
MODEL = IC_FLD(**model_kwargs).to(DEVICE)

# Compile model (PyTorch 2.0+)
if hasattr(torch, 'compile'):
    print("[Optimization] Compiling model with torch.compile()...")
    MODEL = torch.compile(MODEL, mode='reduce-overhead')  # or 'max-autotune'
    print("[Optimization] Model compiled successfully.")
```

**Expected Speedup:** 10-30% faster (after warmup).

**Modes:**
- `default`: Balanced optimization (recommended)
- `reduce-overhead`: Minimize Python overhead
- `max-autotune`: Aggressive optimization (slower compile time)

---

## Data Loading Optimizations

### Optimization 8: DataLoader Prefetching

**Problem:** CPU-GPU data transfer blocks training.

**Solution:** Enable prefetching with pinned memory.

**Before (`parse_datasets.py:80-88`):**
```python
train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True,
    collate_fn=lambda batch: collate_fn(batch, args, device, ...))
```

**After:**
```python
train_dataloader = DataLoader(
    train_data,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=lambda batch: collate_fn(batch, args, device, ...),
    num_workers=4,              # Parallel data loading (tune based on CPU cores)
    pin_memory=True,            # Faster CPU->GPU transfer
    persistent_workers=True,    # Keep workers alive between epochs
    prefetch_factor=2           # Prefetch 2 batches per worker
)
```

**Expected Speedup:** 15-30% reduction in data loading time.

**Tuning:**
- `num_workers`: Start with 4, increase to `min(8, CPU_cores / 2)`
- `prefetch_factor`: 2-4 (higher = more memory, faster loading)

---

### Optimization 9: Cached Min/Max Computation

**Problem:** `get_data_min_max()` recomputes on every run.

**Solution:** Cache results to disk.

**Implementation:**
```python
def get_data_min_max(data, device):
    import pickle
    from hashlib import md5

    # Create cache key based on data identifiers
    cache_key = md5(str([item[0] for item in data[:100]]).encode()).hexdigest()
    cache_path = Path(f".cache/minmax_{cache_key}.pkl")
    cache_path.parent.mkdir(exist_ok=True)

    if cache_path.exists():
        print(f"[Cache] Loading min/max from {cache_path}")
        with open(cache_path, 'rb') as f:
            data_min, data_max, time_max = pickle.load(f)
        return (
            torch.tensor(data_min, device=device),
            torch.tensor(data_max, device=device),
            torch.tensor(time_max, device=device)
        )

    # Original computation
    data_min, data_max, time_max = _compute_data_min_max(data, device)

    # Save to cache
    with open(cache_path, 'wb') as f:
        pickle.dump((data_min.cpu().numpy(), data_max.cpu().numpy(), time_max.item()), f)

    return data_min, data_max, time_max
```

**Expected Speedup:** Eliminates startup delay (1-5 seconds saved per run).

---

## Memory Optimizations

### Optimization 10: Checkpoint Gradients (for deep models)

**Problem:** Storing all activations for backprop consumes memory.

**Solution:** Use gradient checkpointing (trade compute for memory).

**Implementation (in `FLD_ICC.py`):**
```python
from torch.utils.checkpoint import checkpoint

class IC_FLD(nn.Module):
    def __init__(self, ..., use_grad_checkpoint=False):
        super().__init__()
        # ... existing init ...
        self.use_grad_checkpoint = use_grad_checkpoint

    def forward(self, timesteps, X, M, y_times, denorm_time_max=None):
        # ... existing code up to attention ...

        if self.use_grad_checkpoint and self.training:
            # Recompute attention in backward pass (saves memory)
            Z = checkpoint(self.attn, Q, K, V, mask_flat, use_reentrant=False)
        else:
            Z = self.attn(Q, K, V, mask_flat)

        # ... rest of forward pass ...
```

**Expected Benefit:** 30-50% memory reduction (useful for large models or batch sizes).

**Trade-off:** ~10-15% slower training (recomputes activations during backward).

---

### Optimization 11: Empty Cache Between Epochs

**Problem:** Fragmented GPU memory from variable-length sequences.

**Solution:** Clear cache periodically.

**Implementation (add after line 623 in `train_FLD_ICC.py`):**
```python
# After saving latest checkpoint
torch.save({...}, ckpt_last)

# Clear GPU memory fragmentation every 10 epochs
if epoch % 10 == 0 and torch.cuda.is_available():
    torch.cuda.empty_cache()
```

**Expected Benefit:** Prevents OOM errors in long training runs.

---

## Inference Optimizations

### Optimization 12: Batch Evaluation

**Problem:** Evaluation processes one batch at a time with overhead.

**Solution:** Accumulate metrics across batches more efficiently.

**Before (`train_FLD_ICC.py:563-580`):**
```python
def evaluate(loader, nb):
    total = 0.0; total_abs = 0.0; cnt = 0.0
    for _ in range(nb):
        b = utils.get_next_batch(loader)
        T, X, M, TY, Y, YM = batch_to_icfld(b, INPUT_DIM, DEVICE)
        with torch.no_grad():
            YH = MODEL(T, X, M, TY, ...)
        diff = Y - YH
        total += float((YM * (diff) ** 2).sum().item())
        total_abs += float((YM * diff.abs()).sum().item())
        cnt += float(YM.sum().item())
    # ...
```

**After:**
```python
def evaluate(loader, nb):
    # Pre-allocate tensors for accumulation (avoid Python loop overhead)
    total_tensor = torch.tensor(0.0, device=DEVICE)
    total_abs_tensor = torch.tensor(0.0, device=DEVICE)
    cnt_tensor = torch.tensor(0.0, device=DEVICE)

    for _ in range(nb):
        b = utils.get_next_batch(loader)
        T, X, M, TY, Y, YM = batch_to_icfld(b, INPUT_DIM, DEVICE)
        with torch.no_grad(), autocast(device_type=DEVICE.type, dtype=torch.float16):
            YH = MODEL(T, X, M, TY, ...)

        diff = Y - YH
        # Accumulate on GPU (faster than .item() in loop)
        total_tensor += (YM * (diff) ** 2).sum()
        total_abs_tensor += (YM * diff.abs()).sum()
        cnt_tensor += YM.sum()

    # Single CPU transfer at the end
    total = total_tensor.item()
    total_abs = total_abs_tensor.item()
    cnt = cnt_tensor.item()

    mse = total / max(1.0, cnt)
    rmse = (mse + 1e-8) ** 0.5
    mae = total_abs / max(1.0, cnt)
    return {"loss": mse, "mse": mse, "rmse": rmse, "mae": mae}
```

**Expected Speedup:** 10-20% faster evaluation.

---

### Optimization 13: Disable Gradient Computation Globally

**Problem:** `torch.no_grad()` creates context overhead in loops.

**Solution:** Use `@torch.inference_mode()` decorator.

**Before:**
```python
def evaluate(loader, nb):
    for _ in range(nb):
        with torch.no_grad():
            YH = MODEL(...)
```

**After:**
```python
@torch.inference_mode()  # More aggressive than no_grad()
def evaluate(loader, nb):
    for _ in range(nb):
        YH = MODEL(...)  # No context manager needed
```

**Expected Speedup:** 5-10% faster inference (disables view tracking).

---

## Quick Wins Summary

**Implement these first for maximum impact:**

### 1. **Automatic Mixed Precision** (Optimization 5)
- **Effort:** 10 lines of code
- **Speedup:** 1.5-2x
- **Risk:** Low (well-tested in PyTorch)

### 2. **DataLoader Prefetching** (Optimization 8)
- **Effort:** 5 parameters
- **Speedup:** 15-30%
- **Risk:** None

### 3. **Flash Attention** (Optimization 4)
- **Effort:** 15 lines of code
- **Speedup:** 20-40%
- **Risk:** Low (requires PyTorch 2.0+)

### 4. **Batch Evaluation** (Optimization 12)
- **Effort:** 10 lines of code
- **Speedup:** 10-20%
- **Risk:** None

### 5. **Compiled Model** (Optimization 7)
- **Effort:** 2 lines of code
- **Speedup:** 10-30%
- **Risk:** Medium (may have bugs, easy to disable)

---

## Combined Implementation Example

**Minimal changes to `train_FLD_ICC.py` for ~2-3x speedup:**

```python
# After line 375 (setup section)
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))

# After line 448 (model creation)
if hasattr(torch, 'compile'):
    MODEL = torch.compile(MODEL, mode='reduce-overhead')

# Replace training loop (lines 586-594)
for _ in range(num_train_batches):
    optimizer.zero_grad(set_to_none=True)
    batch = utils.get_next_batch(data_obj["train_dataloader"])
    T, X, M, TY, Y, YM = batch_to_icfld(batch, INPUT_DIM, DEVICE)

    with autocast(device_type=DEVICE.type, dtype=torch.float16):
        YH = MODEL(T, X, M, TY, denorm_time_max=(args.time_max_hours if args.use_cycle else None))
        loss = mse_masked(Y, YH, YM)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    last_train_loss = float(loss.item())

# Replace evaluate() with @torch.inference_mode() and GPU accumulation
@torch.inference_mode()
def evaluate(loader, nb):
    total = torch.tensor(0.0, device=DEVICE)
    total_abs = torch.tensor(0.0, device=DEVICE)
    cnt = torch.tensor(0.0, device=DEVICE)

    for _ in range(nb):
        b = utils.get_next_batch(loader)
        T, X, M, TY, Y, YM = batch_to_icfld(b, INPUT_DIM, DEVICE)
        with autocast(device_type=DEVICE.type, dtype=torch.float16):
            YH = MODEL(T, X, M, TY, denorm_time_max=(args.time_max_hours if args.use_cycle else None))
        diff = Y - YH
        total += (YM * diff.pow(2)).sum()
        total_abs += (YM * diff.abs()).sum()
        cnt += YM.sum()

    total, total_abs, cnt = total.item(), total_abs.item(), cnt.item()
    mse = total / max(1.0, cnt)
    return {"loss": mse, "mse": mse, "rmse": (mse + 1e-8) ** 0.5, "mae": total_abs / max(1.0, cnt)}
```

**In `parse_datasets.py` (lines 80-88):**
```python
train_dataloader = DataLoader(
    train_data, batch_size=batch_size, shuffle=True,
    collate_fn=lambda batch: collate_fn(batch, args, device, ...),
    num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2
)
```

---

## Benchmarking Checklist

Before/after testing:

```bash
# Baseline
python FLD_ICC/train_FLD_ICC.py -d physionet -bs 32 --epochs 10

# With optimizations
python FLD_ICC/train_FLD_ICC.py -d physionet -bs 32 --epochs 10 \
  --grad-accum-steps 2  # If using gradient accumulation

# Profile memory usage
python -m torch.utils.bottleneck FLD_ICC/train_FLD_ICC.py -d physionet -bs 32 --epochs 1
```

**Expected results:**
- **Training time per epoch:** 30-50% faster
- **Memory usage:** 20-40% lower (with AMP)
- **Final accuracy:** Same or slightly better (AMP is numerically stable for most models)

---

## Platform-Specific Notes

### CUDA (NVIDIA GPUs)
- All optimizations applicable
- Flash Attention requires Ampere (A100, RTX 30XX+) or newer for best results
- AMP uses Tensor Cores on Volta+ GPUs

### CPU
- Skip AMP (CPU doesn't support fp16 efficiently)
- DataLoader prefetching still helps
- `torch.compile()` works but with smaller gains (5-10%)

### Windows
- `num_workers > 0` may require `if __name__ == '__main__'` guard
- Consider `persistent_workers=False` if you encounter multiprocessing errors

---

## Troubleshooting

### Issue: AMP causes NaN losses
**Solution:** Disable gradient scaling or use higher precision for loss computation:
```python
scaler = GradScaler(enabled=False)  # Disable
# or
with autocast(dtype=torch.bfloat16):  # More stable than float16
```

### Issue: `torch.compile()` fails
**Solution:** Disable with environment variable:
```python
os.environ['TORCH_COMPILE_DISABLE'] = '1'
```

### Issue: DataLoader multiprocessing hangs
**Solution:** Reduce `num_workers` or disable:
```python
train_dataloader = DataLoader(..., num_workers=0)
```

---

## References

- **Flash Attention:** https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html
- **AMP Guide:** https://pytorch.org/docs/stable/amp.html
- **torch.compile():** https://pytorch.org/docs/stable/generated/torch.compile.html
- **DataLoader Best Practices:** https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading

---

**Next Steps:**
1. Implement Quick Wins (Optimizations 5, 8, 4, 12, 7)
2. Benchmark on your hardware
3. Gradually add advanced optimizations (6, 10) as needed
4. Profile with `torch.profiler` for dataset-specific bottlenecks

---

<!-- Source: OPTIMIZATION_README.md -->

# IC-FLD Performance Optimization - Quick Start

This directory contains optimized training scripts and documentation for speeding up IC-FLD training by **2-3x** using Pythonic and PyTorch best practices.

---

## Quick Start (TL;DR)

**Use the optimized trainer instead of the baseline:**

```bash
# Baseline (slower)
python FLD_ICC/train_FLD_ICC.py -d physionet -bs 32 --epochs 100 -fn L -ld 128

# Optimized (2-3x faster)
python FLD_ICC/train_FLD_ICC_optimized.py -d physionet -bs 32 --epochs 100 -fn L -ld 128 \
  --grad-accum-steps 2
```

**Expected improvements:**
- ‚úÖ 1.5-2x faster training per epoch (AMP)
- ‚úÖ 20-40% faster attention (Flash Attention on PyTorch 2.0+)
- ‚úÖ 10-20% faster evaluation (GPU accumulation)
- ‚úÖ 30-40% lower memory usage (mixed precision)

---

## Files Overview

### Documentation
- **`OPTIMIZATION_GUIDE.md`**: Comprehensive guide with 13 optimizations (theory + code)
- **`IC-FLD_Channel_Processing_Analysis.md`**: How channels flow through attention (architectural analysis)
- **`OPTIMIZATION_README.md`**: This file

### Code
- **`FLD_ICC/train_FLD_ICC_optimized.py`**: Drop-in replacement for `train_FLD_ICC.py` with optimizations applied
- **`scripts/benchmark_optimizations.sh`**: Bash script to measure speedup
- **`scripts/benchmark_optimizations.ps1`**: PowerShell script to measure speedup (Windows)

---

## What's Optimized?

The optimized trainer includes:

| Optimization | Speedup | Memory | Effort | Risk |
|--------------|---------|--------|--------|------|
| **Automatic Mixed Precision (AMP)** | 1.5-2x | -40% | Low | Low |
| **Flash Attention (SDPA)** | 1.2-1.4x | -20% | Medium | Low |
| **Gradient Accumulation** | Better GPU utilization | Neutral | Low | None |
| **GPU Tensor Accumulation** | 1.1-1.2x eval | Minimal | Low | None |
| **torch.compile()** | 1.1-1.3x | Neutral | Low | Medium |

**Combined effect: ~2-3x faster training, ~40% lower memory.**

---

## Usage

### Basic Usage (Same as Baseline)

The optimized trainer is a **drop-in replacement** with the same interface:

```bash
python FLD_ICC/train_FLD_ICC_optimized.py \
  -d physionet \
  -ot 24 \
  -bs 32 \
  --epochs 100 \
  --early-stop 15 \
  -fn L \
  -ld 128 \
  -ed 64 \
  -nh 4 \
  --depth 2 \
  --lr 1e-4 \
  --wd 1e-3 \
  --tbon \
  --logdir runs/physionet_opt
```

### Optimization-Specific Flags

```bash
# Gradient accumulation (effective batch size = 32 √ó 4 = 128)
--grad-accum-steps 4

# Disable mixed precision (if you encounter NaN losses)
--no-amp

# Disable torch.compile() (if compilation fails)
--no-compile
```

### Full Example with All Flags

```bash
python FLD_ICC/train_FLD_ICC_optimized.py \
  -d mimic \
  -ot 48 \
  -bs 16 \
  --epochs 1000 \
  --early-stop 20 \
  -fn Q \
  -ld 256 \
  -ed 128 \
  -nh 8 \
  --depth 4 \
  --lr 1e-4 \
  --wd 1e-3 \
  --grad-accum-steps 4 \
  --tbon \
  --logdir runs/mimic_Q_optimized
```

**Effect:**
- Effective batch size: 16 √ó 4 = 64 (better GPU utilization)
- Mixed precision enabled (faster computation)
- torch.compile() enabled (JIT optimization)

---

## Benchmarking

### Run Comparison Benchmark

**Bash/Linux/macOS:**
```bash
chmod +x scripts/benchmark_optimizations.sh
./scripts/benchmark_optimizations.sh
```

**PowerShell/Windows:**
```powershell
.\scripts\benchmark_optimizations.ps1
```

**Custom dataset/config:**
```powershell
.\scripts\benchmark_optimizations.ps1 -Dataset mimic -BatchSize 64 -Epochs 10
```

### Expected Output

```
========================================
Benchmark Results
========================================
Baseline time:   450s
Optimized time:  180s
Speedup:         2.50x
Improvement:     60.0%
```

---

## Troubleshooting

### Issue: NaN losses with mixed precision

**Symptom:** Training diverges, loss becomes NaN.

**Solution 1:** Disable AMP:
```bash
python FLD_ICC/train_FLD_ICC_optimized.py ... --no-amp
```

**Solution 2:** Use bfloat16 (more stable, requires Ampere GPU or newer):
Edit `train_FLD_ICC_optimized.py`, change:
```python
with autocast(device_type=DEVICE.type, dtype=torch.float16, enabled=use_amp):
```
to:
```python
with autocast(device_type=DEVICE.type, dtype=torch.bfloat16, enabled=use_amp):
```

---

### Issue: torch.compile() fails

**Symptom:** Error during model compilation or first forward pass.

**Solution:** Disable compilation:
```bash
python FLD_ICC/train_FLD_ICC_optimized.py ... --no-compile
```

**Note:** `torch.compile()` requires PyTorch 2.0+. If using PyTorch 1.x, it's automatically skipped.

---

### Issue: DataLoader multiprocessing hangs

**Symptom:** Training hangs after "Dataset n_samples" message.

**Solution:** Edit `lib/parse_datasets.py`, set `num_workers=0` in DataLoader calls (lines 80-88, etc.):
```python
train_dataloader = DataLoader(
    train_data,
    batch_size=batch_size,
    shuffle=True,
    collate_fn=...,
    num_workers=0,  # Disable multiprocessing
)
```

---

### Issue: Out of memory (OOM)

**Solution 1:** Reduce batch size:
```bash
python FLD_ICC/train_FLD_ICC_optimized.py -bs 16 --grad-accum-steps 4  # Effective bs=64
```

**Solution 2:** Enable gradient checkpointing (see `OPTIMIZATION_GUIDE.md`, Optimization 10).

**Solution 3:** Reduce model size:
```bash
-ld 64 -ed 32 -nh 2 --depth 2  # Smaller model
```

---

## Performance Tuning by Dataset

### PhysioNet (T=24, C=37)
```bash
# Good starting point
-bs 32 -ld 128 -ed 64 -nh 4 --depth 2 --grad-accum-steps 2
```

### MIMIC (T=48, C=96, larger sequences)
```bash
# Reduce batch size, increase accumulation
-bs 16 -ld 128 -ed 64 -nh 4 --depth 2 --grad-accum-steps 4
```

### USHCN (T=48, C=5, many samples)
```bash
# Can use larger batch size
-bs 64 -ld 64 -ed 32 -nh 4 --depth 2 --grad-accum-steps 1
```

### Activity (T=variable, C=20)
```bash
# Balanced config
-bs 32 -ld 128 -ed 64 -nh 4 --depth 2 --grad-accum-steps 2
```

---

## Advanced: Applying Optimizations to Baseline

If you want to manually apply optimizations to `train_FLD_ICC.py`, see **`OPTIMIZATION_GUIDE.md`** for step-by-step instructions.

**Key sections:**
- **Section 2:** Model architecture optimizations (Flash Attention, etc.)
- **Section 3:** Training loop optimizations (AMP, gradient accumulation)
- **Section 6:** Quick wins summary

---

## What's NOT Changed

The optimized trainer maintains:
- ‚úÖ Same model architecture (IC-FLD)
- ‚úÖ Same hyperparameters (unless explicitly changed)
- ‚úÖ Same evaluation metrics (MSE, RMSE, MAE)
- ‚úÖ Same checkpointing logic
- ‚úÖ Same TensorBoard logging
- ‚úÖ Same FLD/TSDM reporting (if enabled)

**Result:** Comparable or identical final accuracy, just **faster**.

---

## Requirements

**Minimum:**
- PyTorch >= 1.12 (for basic AMP support)
- CUDA 11.0+ (for GPU acceleration)

**Recommended:**
- PyTorch >= 2.0 (for `torch.compile()` and Flash Attention)
- CUDA 11.8+ with Ampere GPU (RTX 30XX, A100, etc.) for best performance

**Check your version:**
```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')"
```

---

## Citation

If you use these optimizations in your research, consider citing:

```bibtex
@software{icfld_optimizations_2025,
  title = {IC-FLD Performance Optimizations},
  author = {[Your Name]},
  year = {2025},
  note = {Pythonic and PyTorch optimizations for Inter-Channel Functional Latent Dynamics}
}
```

---

## Support

**Found a bug or have questions?**
1. Check `OPTIMIZATION_GUIDE.md` for detailed explanations
2. Review the "Troubleshooting" section above
3. Compare logs from `benchmark_optimizations.sh` to diagnose issues

**Want to contribute?**
Additional optimizations are welcome! Potential areas:
- Flash Attention v2 integration (`xformers` or `flash-attn`)
- Custom CUDA kernels for embedding construction
- DeepSpeed integration for multi-GPU training
- Quantization-aware training (QAT)

---

## License

Same as the main IC-FLD repository.

---

<!-- Source: RUNCOMMANDS.md -->

# Run Commands - Exact Commands to Execute

**Copy-paste these commands exactly as shown.**

---

## üîç Step 1: Verify Setup

### Check Python Environment

```powershell
# Activate virtual environment (if using one)
.\.venv\Scripts\activate

# Check Python version
python --version

# Check if datasets exist
python scripts/check_data_paths.py

# Check GPU
nvidia-smi
```

**Expected:** All should work without errors.

---

## üß™ Step 2: Test FLD (5 epochs, ~30-60 seconds)

### Direct FLD Test (Recommended - Shows Output Immediately)

```powershell
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 5 -es 3 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0
```

**What you should see:**
```
Dataset n_samples: 8000 6000 1200 800
Test record ids (first 20): [...]
...
PID, device: XXXXX cuda:0
Dataset=physionet, INPUT_DIM=41, history=24
n_train_batches: 225
- Epoch 001 | train_loss(one-batch): 0.XXXX | val_loss: 0.XXXX | ...
- Epoch 002 | train_loss(one-batch): 0.XXXX | val_loss: 0.XXXX | ...
...
- Epoch 005 | train_loss(one-batch): 0.XXXX | val_loss: 0.XXXX | ...
Best val MSE: 0.XXXX @ epoch X
{"best_epoch": X, "val_mse_best": 0.XXXX, ...}
```

**Duration:** 30-60 seconds

**If it fails:** Check error message and see Troubleshooting section below.

---

## üß™ Step 3: Test IC-FLD (5 epochs, ~30-60 seconds)

### Direct IC-FLD Test

```powershell
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 5 --early-stop 3 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0
```

**What you should see:**
```
Dataset n_samples: 8000 6000 1200 800
...
PID, device: XXXXX cuda:0
Dataset=physionet, INPUT_DIM=41, history=24
n_train_batches: 225
- Epoch 001 | train_loss(one-batch): 0.XXXX | val_loss: 0.XXXX | ...
- Epoch 002 | train_loss(one-batch): 0.XXXX | val_loss: 0.XXXX | ...
...
- Epoch 005 | train_loss(one-batch): 0.XXXX | val_loss: 0.XXXX | ...
Best val MSE: 0.XXXX @ epoch X
{"best_epoch": X, "val_mse_best": 0.XXXX, ...}
```

**Duration:** 30-60 seconds

---

## üöÄ Step 4: Run Quick Benchmark (100 epochs)

### PowerShell

```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100 -BatchSize 32 -EarlyStop 15
```

### Alternative: Run Single Function Manually

**FLD - Linear Basis (100 epochs):**
```powershell
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 100 -es 15 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0 --tbon --logdir runs/FLD_L_physionet 2>&1 | Tee-Object -FilePath "logs/FLD_L.log"
```

**IC-FLD - Linear Basis (100 epochs):**
```powershell
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 100 --early-stop 15 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0 --tbon --logdir runs/ICFLD_L_physionet 2>&1 | Tee-Object -FilePath "logs/ICFLD_L.log"
```

**Create logs directory first:**
```powershell
New-Item -ItemType Directory -Force -Path logs
```

**Duration:** ~15-30 minutes per model

---

## üìä Step 5: Monitor Progress

### Watch Log File (Real-Time)

**PowerShell:**
```powershell
Get-Content -Path "logs/FLD_L.log" -Wait -Tail 50
```

**Or use TensorBoard:**
```powershell
tensorboard --logdir runs
```

Then open: http://localhost:6006

---

## üéØ Full Benchmark Commands (All Functions, 1000 Epochs)

### Option 1: Run Full Benchmark Script

```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1
```

**Duration:** 8-16 hours (8 runs √ó 1000 epochs each)

---

### Option 2: Run Individual Commands (Manual Control)

Create logs directory:
```powershell
New-Item -ItemType Directory -Force -Path logs
```

#### FLD - All Functions

```powershell
# FLD - Constant
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 1000 -es 15 -fn C -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0 --tbon --logdir runs/FLD_C_physionet 2>&1 | Tee-Object -FilePath "logs/FLD_C.log"

# FLD - Linear
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 1000 -es 15 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0 --tbon --logdir runs/FLD_L_physionet 2>&1 | Tee-Object -FilePath "logs/FLD_L.log"

# FLD - Quadratic
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 1000 -es 15 -fn Q -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0 --tbon --logdir runs/FLD_Q_physionet 2>&1 | Tee-Object -FilePath "logs/FLD_Q.log"

# FLD - Sinusoidal
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 1000 -es 15 -fn S -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0 --tbon --logdir runs/FLD_S_physionet 2>&1 | Tee-Object -FilePath "logs/FLD_S.log"
```

#### IC-FLD - All Functions

```powershell
# IC-FLD - Constant
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 1000 --early-stop 15 -fn C -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0 --tbon --logdir runs/ICFLD_C_physionet 2>&1 | Tee-Object -FilePath "logs/ICFLD_C.log"

# IC-FLD - Linear
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 1000 --early-stop 15 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0 --tbon --logdir runs/ICFLD_L_physionet 2>&1 | Tee-Object -FilePath "logs/ICFLD_L.log"

# IC-FLD - Quadratic
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 1000 --early-stop 15 -fn Q -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0 --tbon --logdir runs/ICFLD_Q_physionet 2>&1 | Tee-Object -FilePath "logs/ICFLD_Q.log"

# IC-FLD - Sinusoidal
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 1000 --early-stop 15 -fn S -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0 --tbon --logdir runs/ICFLD_S_physionet 2>&1 | Tee-Object -FilePath "logs/ICFLD_S.log"
```

---

## üìù Extract Metrics After Manual Runs

### Get Final Metrics from Logs

```powershell
# Get JSON from FLD log
Get-Content logs/FLD_L.log | Select-String '^\{.*\}$' | Select-Object -Last 1

# Get JSON from IC-FLD log
Get-Content logs/ICFLD_L.log | Select-String '^\{.*\}$' | Select-Object -Last 1

# Get best epoch info
Get-Content logs/FLD_L.log | Select-String "Best val MSE"
Get-Content logs/ICFLD_L.log | Select-String "Best val MSE"
```

---

## üõ†Ô∏è Troubleshooting

### Issue: Command not found / Module not found

```powershell
# Activate virtual environment
.\.venv\Scripts\activate

# Verify Python
python --version

# Reinstall dependencies
pip install -r requirements.txt
```

---

### Issue: CUDA out of memory

**Reduce batch size:**

```powershell
# FLD with smaller batch
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 16 -e 5 -es 3 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0

# IC-FLD with smaller batch
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 16 --epochs 5 --early-stop 3 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0
```

---

### Issue: Dataset not found

```powershell
# Check if datasets exist
python scripts/check_data_paths.py

# If missing, check data directory structure
Get-ChildItem -Path data -Recurse
```

Expected structure:
```
data/
‚îú‚îÄ‚îÄ physionet/
‚îÇ   ‚îú‚îÄ‚îÄ set-a/
‚îÇ   ‚îî‚îÄ‚îÄ set-b/
‚îú‚îÄ‚îÄ mimic/
‚îú‚îÄ‚îÄ ushcn/
‚îî‚îÄ‚îÄ activity/
```

---

### Issue: No output or very slow start

**Normal delays (don't panic!):**
- Dataset loading: 10-30 seconds first time
- CUDA initialization: 5-10 seconds
- First epoch: Slower than subsequent epochs

**If stuck for >2 minutes:**
```powershell
# Press Ctrl+C to stop
# Try with fewer epochs
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 2 -es 1 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0
```

---

### Issue: Logs not showing in real-time

**PowerShell alternative (shows immediately):**

```powershell
# Run without Tee-Object to see output directly
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 5 -es 3 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0
```

**Or redirect only errors to see stdout immediately:**
```powershell
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 5 -es 3 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0 *>&1 | Out-Default
```

---

## ‚úÖ Quick Validation Checklist

Run each command and check:

### 1. FLD Quick Test
```powershell
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 2 -es 1 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0
```
- [ ] Runs without errors
- [ ] Shows epoch progress
- [ ] Outputs JSON at end
- [ ] Takes 15-30 seconds

### 2. IC-FLD Quick Test
```powershell
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 2 --early-stop 1 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0
```
- [ ] Runs without errors
- [ ] Shows epoch progress
- [ ] Outputs JSON at end
- [ ] Takes 15-30 seconds

### 3. Ready for Full Benchmark

If both tests pass, you're ready to run:
```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

---

## üéØ Recommended Workflow

### Day 1: Quick Tests
```powershell
# Test both models (5 epochs each, ~2 minutes total)
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 5 -es 3 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0

python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 5 --early-stop 3 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0
```

### Day 2: Medium Test
```powershell
# 100 epochs, ~1-2 hours
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

### Day 3+: Full Benchmark
```powershell
# 1000 epochs, ~8-16 hours
.\scripts\benchmark_fld_vs_icfld_physionet.ps1
```

---

## üìä Expected Timing

| Command | Epochs | Duration | GPU |
|---------|--------|----------|-----|
| Quick test (FLD) | 2 | 15-30s | Any |
| Quick test (IC-FLD) | 2 | 15-30s | Any |
| Quick test (both) | 5 | 1-2 min | Any |
| Single function | 100 | 15-30 min | GTX 1060+ |
| All functions (8 runs) | 100 | 2-4 hours | GTX 1060+ |
| All functions (8 runs) | 1000 | 8-16 hours | GTX 1060+ |
| All functions (8 runs) | 1000 | 4-8 hours | RTX 3080+ |

---

## üîó Related Files

- **`FIXES_APPLIED.md`** - What was broken and how it's fixed
- **`LOGGING_FIXES.md`** - Why logging didn't work and how it's fixed now
- **`BENCHMARK_GUIDE.md`** - Complete benchmark guide with analysis
- **`BENCHMARK_QUICKSTART.md`** - TL;DR version

---

## üÜò If Nothing Works

### Nuclear Option: Fresh Start

```powershell
# 1. Deactivate environment
deactivate

# 2. Delete virtual environment
Remove-Item -Recurse -Force .venv

# 3. Create new environment
python -m venv .venv

# 4. Activate
.\.venv\Scripts\activate

# 5. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 6. Install PyTorch separately
pip install --no-cache-dir torch==2.0.0+cu118 torchvision==0.15.1+cu118 --extra-index-url https://download.pytorch.org/whl/cu118

# 7. Test again
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 2 -es 1 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0
```

---

**Start with Step 1 and work your way down. Good luck! üöÄ**

---

<!-- Source: TIMING_LOGS_ADDED.md -->

# Comprehensive Timing Logs - Implementation Complete

**Date:** 2025-10-25
**Changes:** Added detailed timing logs using Python's logging module to all trainers

---

## ‚úÖ Files Modified

1. **`FLD/train_FLD.py`** - FLD Baseline Trainer
2. **`FLD_ICC/train_FLD_ICC.py`** - IC-FLD Trainer
3. **(Pending) `FLD_ICC/train_FLD_ICC_ushcn.py`** - USHCN-specific Trainer

---

## üìä Logging Format

All logs use Python's `logging` module with timestamps:

```
[2025-10-25 14:30:22] INFO: ================================================================================
[2025-10-25 14:30:22] INFO: FLD TRAINING STARTED
[2025-10-25 14:30:22] INFO: Script: FLD/train_FLD.py
[2025-10-25 14:30:22] INFO: Start time: 2025-10-25 14:30:22
[2025-10-25 14:30:22] INFO: ================================================================================
[2025-10-25 14:30:25] INFO: Loading dataset...
[2025-10-25 14:30:42] INFO: Dataset loaded in 17.34 seconds
[2025-10-25 14:30:42] INFO: Dataset: physionet, Input Dim: 41, History: 24, Train Batches: 225
[2025-10-25 14:30:42] INFO: Creating FLD model...
[2025-10-25 14:30:45] INFO: Model created in 2.87 seconds
[2025-10-25 14:30:45] INFO: Model: FLD(input_dim=41, latent_dim=20, function=L, heads=4, depth=2)
[2025-10-25 14:30:45] INFO: ================================================================================
[2025-10-25 14:30:45] INFO: TRAINING LOOP STARTED
[2025-10-25 14:30:45] INFO: Epochs: 100, Early Stop: 15, Batch Size: 32
[2025-10-25 14:30:45] INFO: Learning Rate: 0.0001, Weight Decay: 0.001
[2025-10-25 14:30:45] INFO: ================================================================================
... (epoch logs) ...
[2025-10-25 14:45:18] INFO: ================================================================================
[2025-10-25 14:45:18] INFO: TRAINING LOOP COMPLETED
[2025-10-25 14:45:18] INFO: Training duration: 873.24 seconds (14.55 minutes)
[2025-10-25 14:45:18] INFO: Best epoch: 67, Best val MSE: 0.006234
[2025-10-25 14:45:18] INFO: ================================================================================
[2025-10-25 14:45:20] INFO: ================================================================================
[2025-10-25 14:45:20] INFO: FLD TRAINING COMPLETED SUCCESSFULLY
[2025-10-25 14:45:20] INFO: End time: 2025-10-25 14:45:20
[2025-10-25 14:45:20] INFO: Total script duration: 898.45 seconds (14.97 minutes)
[2025-10-25 14:45:20] INFO:   - Dataset loading: 17.34s
[2025-10-25 14:45:20] INFO:   - Model creation: 2.87s
[2025-10-25 14:45:20] INFO:   - Training loop: 873.24s (14.55 min)
[2025-10-25 14:45:20] INFO:   - Other overhead: 5.00s
[2025-10-25 14:45:20] INFO: ================================================================================
```

---

## üîç Timing Breakdown

### What's Logged

1. **Script Start**
   - Timestamp
   - Script path

2. **Dataset Loading**
   - Start message
   - Duration
   - Dataset info (name, dimensions, batches)

3. **Model Creation**
   - Start message
   - Duration
   - Model configuration

4. **Training Loop Start**
   - Configuration summary (epochs, batch size, learning rate, etc.)

5. **Training Progress** (existing per-epoch logs)
   - Each epoch's metrics
   - Time per epoch

6. **Early Stopping** (if triggered)
   - Which epoch stopped
   - Reason

7. **Training Loop End**
   - Total training duration
   - Best epoch and MSE

8. **Script End**
   - Total script duration
   - Breakdown by component:
     - Dataset loading time
     - Model creation time
     - Training loop time
     - Overhead (initialization, saving, etc.)

---

## üß™ Test the New Logging

### Quick Test (2 epochs)

```powershell
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 2 -es 1 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0
```

**Expected output:**
```
[2025-10-25 14:30:22] INFO: ================================================================================
[2025-10-25 14:30:22] INFO: FLD TRAINING STARTED
...
[2025-10-25 14:30:42] INFO: Dataset loaded in 17.34 seconds
...
[2025-10-25 14:30:45] INFO: Model created in 2.87 seconds
...
[2025-10-25 14:30:45] INFO: TRAINING LOOP STARTED
...
- Epoch 001 | train_loss(one-batch): 0.028510 | val_loss: 0.028467 | ... | time: 8.85s
- Epoch 002 | train_loss(one-batch): 0.010109 | val_loss: 0.011775 | ... | time: 9.11s
...
[2025-10-25 14:31:05] INFO: TRAINING LOOP COMPLETED
[2025-10-25 14:31:05] INFO: Training duration: 20.15 seconds (0.34 minutes)
...
[2025-10-25 14:31:07] INFO: Total script duration: 45.23 seconds (0.75 minutes)
[2025-10-25 14:31:07] INFO:   - Dataset loading: 17.34s
[2025-10-25 14:31:07] INFO:   - Model creation: 2.87s
[2025-10-25 14:31:07] INFO:   - Training loop: 20.15s (0.34 min)
[2025-10-25 14:31:07] INFO:   - Other overhead: 4.87s
```

---

### Test IC-FLD

```powershell
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 2 --early-stop 1 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0
```

**Expected output:**
```
[2025-10-25 14:35:00] INFO: ================================================================================
[2025-10-25 14:35:00] INFO: IC-FLD TRAINING STARTED
...
[2025-10-25 14:35:18] INFO: Dataset loaded in 15.67 seconds
...
[2025-10-25 14:35:21] INFO: Model created in 3.12 seconds
...
[2025-10-25 14:35:21] INFO: TRAINING LOOP STARTED
...
[2025-10-25 14:35:45] INFO: TRAINING LOOP COMPLETED
...
[2025-10-25 14:35:47] INFO: IC-FLD TRAINING COMPLETED SUCCESSFULLY
[2025-10-25 14:35:47] INFO: Total script duration: 47.89 seconds (0.80 minutes)
[2025-10-25 14:35:47] INFO:   - Dataset loading: 15.67s
[2025-10-25 14:35:47] INFO:   - Model creation: 3.12s
[2025-10-25 14:35:47] INFO:   - Training loop: 24.35s (0.41 min)
[2025-10-25 14:35:47] INFO:   - Other overhead: 4.75s
```

---

## üìÅ Log Files

### Logs are visible in:

1. **Console/Terminal** - Real-time output
2. **Log files** (if using `Tee-Object` or `tee`):
   ```powershell
   python -u FLD/train_FLD.py ... 2>&1 | Tee-Object -FilePath "logs/FLD_L.log"
   ```

3. **Benchmark script logs** - Automatically saved when using benchmark scripts

---

## üéØ Benefits

### Before (No Detailed Timing)
```
Dataset n_samples: 8000 6000 1200 800
...
PID, device: 18816 cuda
- Epoch 001 | ...
- Epoch 002 | ...
...
Best val MSE: 0.006234 @ epoch 67
```

**Problems:**
- ‚ùå No timestamps
- ‚ùå No component timing breakdown
- ‚ùå Hard to identify bottlenecks
- ‚ùå Can't see if dataset loading is slow

### After (With Detailed Timing)
```
[2025-10-25 14:30:22] INFO: FLD TRAINING STARTED
[2025-10-25 14:30:42] INFO: Dataset loaded in 17.34 seconds
[2025-10-25 14:30:45] INFO: Model created in 2.87 seconds
[2025-10-25 14:30:45] INFO: TRAINING LOOP STARTED
...
[2025-10-25 14:45:20] INFO: Total script duration: 898.45 seconds (14.97 minutes)
[2025-10-25 14:45:20] INFO:   - Dataset loading: 17.34s
[2025-10-25 14:45:20] INFO:   - Model creation: 2.87s
[2025-10-25 14:45:20] INFO:   - Training loop: 873.24s (14.55 min)
[2025-10-25 14:45:20] INFO:   - Other overhead: 5.00s
```

**Benefits:**
- ‚úÖ Precise timestamps for every phase
- ‚úÖ Component-wise timing breakdown
- ‚úÖ Easy to identify bottlenecks
- ‚úÖ Can optimize slow components
- ‚úÖ Know exactly when script started/ended
- ‚úÖ Total time visible at the end

---

## üî¨ Analyzing Timing

### Identify Bottlenecks

**Example log:**
```
[2025-10-25 14:35:47] INFO: Total script duration: 898.45 seconds (14.97 minutes)
[2025-10-25 14:35:47] INFO:   - Dataset loading: 17.34s (1.9%)
[2025-10-25 14:35:47] INFO:   - Model creation: 2.87s (0.3%)
[2025-10-25 14:35:47] INFO:   - Training loop: 873.24s (97.2%)
[2025-10-25 14:35:47] INFO:   - Other overhead: 5.00s (0.6%)
```

**Analysis:**
- ‚úÖ Training loop: 97.2% - Expected, this is where the work happens
- ‚úÖ Dataset loading: 1.9% - Acceptable
- ‚úÖ Model creation: 0.3% - Very fast
- ‚úÖ Overhead: 0.6% - Minimal

**If you see:**
```
[2025-10-25 14:35:47] INFO:   - Dataset loading: 450.00s (50%)
[2025-10-25 14:35:47] INFO:   - Training loop: 430.00s (48%)
```

**Action:** Dataset loading is too slow! Check:
- Is data cached?
- Are there too many workers?
- Is disk I/O slow?

---

## üìä Comparing FLD vs IC-FLD

With these logs, you can easily compare:

```
FLD (100 epochs):
  - Total: 898.45s (14.97 min)
  - Training: 873.24s (14.55 min)

IC-FLD (100 epochs):
  - Total: 1156.78s (19.28 min)
  - Training: 1128.34s (18.81 min)

Difference: IC-FLD is 1.29x slower (expected due to larger model)
```

---

## üöÄ Next Steps

### 1. Test both trainers:

```powershell
# FLD
python -u FLD/train_FLD.py -d physionet -ot 24 -bs 32 -e 2 -es 1 -fn L -ed 4 -nh 4 -dp 2 -lr 1e-4 -wd 1e-3 -s 42 --gpu 0

# IC-FLD
python -u FLD_ICC/train_FLD_ICC.py -d physionet -ot 24 -bs 32 --epochs 2 --early-stop 1 -fn L -ld 128 -ed 64 -nh 4 --depth 2 --lr 1e-4 --wd 1e-3 --seed 42 --gpu 0
```

### 2. Verify timing logs appear

### 3. Run full benchmark:

```powershell
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

### 4. Check benchmark logs for timing breakdown

---

## üìù Checklist

- [x] FLD trainer has detailed logging
- [x] IC-FLD trainer has detailed logging
- [ ] USHCN trainer has detailed logging (if needed)
- [ ] Test FLD with 2 epochs
- [ ] Test IC-FLD with 2 epochs
- [ ] Verify timing breakdown appears
- [ ] Run full benchmark

---

## ‚úÖ Summary

**What was added:**
- Python `logging` module with timestamps
- Timing for:
  - Script start/end
  - Dataset loading
  - Model creation
  - Training loop
  - Component breakdown
- Clear formatting with separator lines

**Result:**
- You now have **complete visibility** into where time is spent
- Easy to identify bottlenecks
- Professional logging format
- All timing information visible at the end

**Try it now!** Run the quick test commands above and see the detailed timing logs in action.

---

<!-- Source: FIXES_APPLIED.md -->

# Critical Fixes Applied - FLD Training

## Issue Identified

FLD training was failing silently (completed in ~4 seconds with no metrics), causing benchmark script failures.

---

## Root Causes Found

### 1. **Syntax Error in `FLD/train_FLD.py` (Line 72)**

**Problem:**
```python
lr=args.learn-rate if hasattr(args, "learn-rate") else args.learn_rate,
```

- `args.learn-rate` is **invalid Python syntax** (hyphen treated as minus operator)
- argparse converts `--learn-rate` to `args.learn_rate` (underscore, not hyphen)
- This caused the script to crash immediately

**Fix:**
```python
lr=args.learn_rate,
```

**File:** `FLD/train_FLD.py:72`

---

### 2. **Wrong Arguments in Benchmark Scripts**

**Problems:**
1. Using `-ld` for FLD (FLD doesn't have this argument, hardcodes `latent_dim=20`)
2. Using `--depth` instead of `-dp` for FLD
3. Using short forms (`-e`, `-es`, `-s`) for both FLD and IC-FLD
   - FLD uses: `-e`, `-es`, `-s` (short forms)
   - IC-FLD uses: `--epochs`, `--early-stop`, `--seed` (long forms only)

**Fixes Applied:**

#### PowerShell Script (`benchmark_fld_vs_icfld_physionet.ps1`)

**Before:**
```powershell
# Common arguments (used for BOTH models - incorrect!)
$Args = @(
    ...
    "--epochs", $Epochs,      # IC-FLD only
    "--early-stop", $EarlyStop,  # IC-FLD only
    "-fn", $Function,
    "--lr", $LearningRate,
    "--wd", $WeightDecay,
    "--seed", $Seed,          # IC-FLD only
    ...
)

# FLD-specific
-ExtraArgs @{
    "-ld" = "20"      # INVALID - FLD doesn't accept -ld
    "-ed" = "4"
    "-nh" = "4"
    "--depth" = "2"   # Should be -dp
}
```

**After:**
```powershell
# Conditional arguments based on trainer
if ($TrainerScript -like "*FLD/train_FLD.py") {
    # FLD format
    $Args = @(
        ...
        "-e", $Epochs,
        "-es", $EarlyStop,
        "-fn", $Function,
        "-lr", $LearningRate,
        "-wd", $WeightDecay,
        "-s", $Seed,
        ...
    )
} else {
    # IC-FLD format
    $Args = @(
        ...
        "--epochs", $Epochs,
        "--early-stop", $EarlyStop,
        "-fn", $Function,
        "--lr", $LearningRate,
        "--wd", $WeightDecay,
        "--seed", $Seed,
        ...
    )
}

# FLD-specific (corrected)
-ExtraArgs @{
    "-ed" = "4"       # Embedding dim per head
    "-nh" = "4"       # Number of heads
    "-dp" = "2"       # Decoder depth (correct short form)
}
```

#### Bash Script (`benchmark_fld_vs_icfld_physionet.sh`)

Same fixes applied with bash syntax.

---

## Argument Reference

### FLD (`FLD/train_FLD.py`)

| Argument | Short | Long | Default | Description |
|----------|-------|------|---------|-------------|
| Dataset | `-d` | `--dataset` | `ushcn` | Dataset name |
| Observation Time | `-ot` | `--observation-time` | `24` | History window |
| Batch Size | `-bs` | `--batch-size` | `64` | Batch size |
| Epochs | `-e` | `--epochs` | `300` | Training epochs |
| Early Stop | `-es` | `--early-stop` | `30` | Patience |
| Function | `-fn` | `--function` | `C` | Basis function |
| Learning Rate | `-lr` | `--learn-rate` | `1e-3` | Learning rate |
| Weight Decay | `-wd` | `--weight-decay` | `0.0` | L2 regularization |
| Seed | `-s` | `--seed` | `0` | Random seed |
| Embedding Dim | `-ed` | `--embedding-dim` | `4` | Embed dim per head |
| Num Heads | `-nh` | `--num-heads` | `2` | Attention heads |
| Depth | `-dp` | `--depth` | `1` | Decoder depth |
| GPU | | `--gpu` | `0` | GPU device |
| Resume | | `--resume` | `""` | Checkpoint path |
| TensorBoard | | `--tbon` | `False` | Enable logging |
| Log Dir | | `--logdir` | `runs` | TB directory |

**Note:** FLD hardcodes `latent_dim=20` (not configurable via CLI)

### IC-FLD (`FLD_ICC/train_FLD_ICC.py`)

| Argument | Short | Long | Default | Description |
|----------|-------|------|---------|-------------|
| Dataset | `-d` | `--dataset` | `physionet` | Dataset name |
| Observation Time | `-ot` | `--observation-time` | `24` | History window |
| Batch Size | `-bs` | `--batch-size` | `32` | Batch size |
| Epochs | | `--epochs` | `100` | Training epochs |
| Early Stop | | `--early-stop` | `10` | Patience |
| Function | `-fn` | `--function` | `L` | Basis function |
| Learning Rate | | `--lr` | `1e-3` | Learning rate |
| Weight Decay | | `--wd` | `0.0` | L2 regularization |
| Seed | | `--seed` | `0` | Random seed |
| Embedding Dim | `-ed` | `--embedding-dim` | `64` | Total embed dim |
| Num Heads | `-nh` | `--num-heads` | `2` | Attention heads |
| Latent Dim | `-ld` | `--latent-dim` | `64` | Latent dimension |
| Depth | | `--depth` | `2` | Decoder depth |
| GPU | | `--gpu` | `0` | GPU device |
| Resume | | `--resume` | `""` | Checkpoint path |
| TensorBoard | | `--tbon` | `False` | Enable logging |
| Log Dir | | `--logdir` | `runs` | TB directory |

---

## Testing the Fixes

### Quick FLD Test (5 epochs)

```powershell
# PowerShell
.\scripts\test_fld_quick.ps1
```

Expected output:
- Should run for ~30-60 seconds (not 4 seconds!)
- Should print training progress
- Should complete with exit code 0

### Quick IC-FLD Test (5 epochs)

```powershell
python FLD_ICC/train_FLD_ICC.py `
  -d physionet `
  -ot 24 `
  -bs 32 `
  --epochs 5 `
  --early-stop 3 `
  -fn L `
  -ld 128 `
  -ed 64 `
  -nh 4 `
  --depth 2 `
  --lr 1e-4 `
  --wd 1e-3 `
  --seed 42 `
  --gpu 0
```

### Full Benchmark (After Verification)

```powershell
# Only run after quick tests succeed!
.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100
```

---

## Files Modified

1. ‚úÖ **`FLD/train_FLD.py`** - Fixed line 72 syntax error
2. ‚úÖ **`scripts/benchmark_fld_vs_icfld_physionet.ps1`** - Fixed argument names and conditionals
3. ‚úÖ **`scripts/benchmark_fld_vs_icfld_physionet.sh`** - Fixed argument names and conditionals
4. ‚úÖ **`scripts/test_fld_quick.ps1`** - Created quick test script

---

## Summary

**What was broken:**
- FLD training crashed immediately due to syntax error
- Benchmark scripts used wrong argument names
- Scripts didn't account for different argument formats between FLD and IC-FLD

**What's fixed:**
- FLD syntax error corrected
- Benchmark scripts now use correct arguments for each model
- Conditional argument handling based on trainer type
- Test script to verify FLD works

**Next steps:**
1. Run `.\scripts\test_fld_quick.ps1` to verify FLD works
2. Run quick IC-FLD test (5 epochs) to verify IC-FLD works
3. Run `.\scripts\benchmark_fld_vs_icfld_physionet.ps1 -Epochs 100` for full test
4. If successful, run full 1000-epoch benchmark

---

## Expected Behavior Now

### FLD Training (5 epochs, ~30-60 seconds)
```
PID, device: 12345 cuda:0
Dataset=physionet, INPUT_DIM=37, history=24
n_train_batches: 51
- Epoch 001 | train_loss: 0.234567 | val_loss: 0.198765 | ...
- Epoch 002 | train_loss: 0.189012 | val_loss: 0.156789 | ...
...
- Epoch 005 | train_loss: 0.098765 | val_loss: 0.087654 | ...
Best val MSE: 0.087654 @ epoch 5
{"best_epoch": 5, "val_mse_best": 0.087654, ...}
```

### Benchmark Script (100 epochs per function, ~8 runs)
```
[1/8 - 12.5%] Running FLD with function 'C'...
Start time: 2025-10-25 14:30:22
...
End time:   2025-10-25 14:45:18
Duration:   896 seconds (14.93 minutes)  ‚úÖ REALISTIC TIME

Key Metrics:
  Best Epoch:       67
  Val MSE (best):   0.012345
  Test MSE (best):  0.013456
  Total Time:       896 seconds
```

---

## Verification Checklist

- [ ] `.\scripts\test_fld_quick.ps1` runs successfully (takes 30-60s, not 4s)
- [ ] FLD prints training progress (epoch logs visible)
- [ ] FLD outputs JSON metrics at the end
- [ ] IC-FLD quick test runs successfully
- [ ] Benchmark script completes at least 1 FLD run successfully
- [ ] Benchmark script completes at least 1 IC-FLD run successfully
- [ ] Logs contain expected metrics (MSE, RMSE, MAE)
- [ ] TensorBoard logs are created

---

**Status:** ‚úÖ Fixes applied and ready for testing
